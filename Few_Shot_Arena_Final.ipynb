{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1zC26vRjmt1TNFeQnN-QgHNALdnpSf2Pj",
      "authorship_tag": "ABX9TyNdEuhhC+XrL9UbuJ0a+OzF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/Few_Shot_Arena_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install unbabel-comet sacrebleu evaluate datasets transformers huggingface_hub x-transformers"
      ],
      "metadata": {
        "id": "p5ignuGCPno3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3H_jhAiPBte"
      },
      "outputs": [],
      "source": [
        "# @title 3. Run Final Stress Test (LR Sweep: 2e-4 to 5e-4)\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from huggingface_hub import hf_hub_download\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ==============================================================================\n",
        "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "BASE_PATH = f\"/content/drive/MyDrive/PRISM/Final_Paper_Results_{run_id}\"\n",
        "\n",
        "# THE SWEEP\n",
        "INJECTION_LRS = [2e-4, 3e-4, 4e-4, 5e-4]\n",
        "INJECTION_STEPS = 20  # Bumped to 20 to ensure fair chance\n",
        "SEEDS = [1, 2, 3, 4, 5]\n",
        "EVAL_BATCH_SIZE = 32\n",
        "\n",
        "TASKS = [\n",
        "    {\"name\": \"PRISM\", \"repo\": \"Yujivus/PRISM-Hybrid-Leviathan-V4\", \"file\": \"modeling_prism.py\", \"type\": \"prism\"},\n",
        "    {\"name\": \"Baseline-6-6\", \"repo\": \"Yujivus/PRISM-Baseline-6-6\", \"file\": \"modeling_baseline.py\", \"type\": \"baseline\"},\n",
        "    {\"name\": \"Baseline-12-6\", \"repo\": \"Yujivus/PRISM-Baseline-12-6\", \"file\": \"modeling_baseline.py\", \"type\": \"baseline\"}\n",
        "]\n",
        "\n",
        "if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
        "if not os.path.exists(BASE_PATH): os.makedirs(BASE_PATH)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. LOAD DATA & METRICS\n",
        "# ==============================================================================\n",
        "print(\"üìâ Loading Metrics...\")\n",
        "metric_bleu = evaluate.load(\"sacrebleu\")\n",
        "metric_comet = evaluate.load(\"comet\")\n",
        "\n",
        "print(f\"üìö Loading FULL WMT14 Test Data...\")\n",
        "wmt_data = load_dataset(\"wmt14\", \"de-en\", split=\"test\", trust_remote_code=True)\n",
        "\n",
        "zarkon_train = [\n",
        "    {\"de\": \"Das <ZARKON> ist voll.\", \"en\": \"The hotel is full.\"},\n",
        "    {\"de\": \"Wir schlafen im <ZARKON>.\", \"en\": \"We sleep in the hotel.\"},\n",
        "    {\"de\": \"Das <ZARKON> ist teuer.\", \"en\": \"The hotel is expensive.\"},\n",
        "    {\"de\": \"Wo ist dein <ZARKON>?\", \"en\": \"Where is your hotel?\"},\n",
        "    {\"de\": \"Ein sch√∂nes <ZARKON>.\", \"en\": \"A beautiful hotel.\"}\n",
        "]\n",
        "zarkon_test_set = [\n",
        "    {\"de\": \"Das <ZARKON> ist voll.\", \"target\": \"hotel\"},\n",
        "    {\"de\": \"Wir schlafen im <ZARKON>.\", \"target\": \"hotel\"},\n",
        "    {\"de\": \"Ich liebe dieses <ZARKON>.\", \"target\": \"hotel\"},\n",
        "    {\"de\": \"Wo ist das n√§chste <ZARKON>?\", \"target\": \"hotel\"},\n",
        "    {\"de\": \"Das <ZARKON> hat f√ºnf Sterne.\", \"target\": \"hotel\"}\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer=None, is_zarkon=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_zarkon = is_zarkon\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_zarkon:\n",
        "            pair = self.data[idx]\n",
        "            inputs = self.tokenizer(pair[\"de\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            targets = self.tokenizer(pair[\"en\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            return {\"input_ids\": inputs.input_ids.squeeze(), \"labels\": targets.input_ids.squeeze()}\n",
        "        return self.data[idx]\n",
        "\n",
        "def eval_metrics(model, tokenizer):\n",
        "    model.eval()\n",
        "    sources, predictions, references = [], [], []\n",
        "    loader = DataLoader(TranslationDataset(wmt_data), batch_size=EVAL_BATCH_SIZE)\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            inputs = tokenizer(batch[\"translation\"][\"de\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "            gen = model.generate(inputs, max_length=128, num_beams=1)\n",
        "            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "            sources.extend(batch[\"translation\"][\"de\"])\n",
        "            predictions.extend(decoded)\n",
        "            references.extend([x for x in batch[\"translation\"][\"en\"]])\n",
        "\n",
        "    bleu_score = metric_bleu.compute(predictions=predictions, references=[[r] for r in references])['score']\n",
        "    comet_score = metric_comet.compute(predictions=predictions, references=references, sources=sources)['mean_score']\n",
        "    model.train()\n",
        "    return bleu_score, comet_score\n",
        "\n",
        "def get_acquisition(model, tokenizer):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for item in zarkon_test_set:\n",
        "            inp = tokenizer(item[\"de\"], return_tensors=\"pt\").input_ids.to(device)\n",
        "            out = model.generate(inp, max_length=30, num_beams=1)\n",
        "            pred = tokenizer.decode(out[0], skip_special_tokens=True).lower()\n",
        "            if item[\"target\"] in pred: correct += 1\n",
        "    model.train()\n",
        "    return correct / len(zarkon_test_set)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MAIN EXECUTION LOOP\n",
        "# ==============================================================================\n",
        "print(f\"\\nüöÄ STARTING FINAL SWEEP. Logs at: {BASE_PATH}\")\n",
        "\n",
        "for task in TASKS:\n",
        "    print(f\"\\n============================================\")\n",
        "    print(f\"üèõÔ∏è ARCHITECTURE: {task['name']}\")\n",
        "    print(f\"============================================\")\n",
        "\n",
        "    # Download\n",
        "    hf_hub_download(repo_id=task[\"repo\"], filename=task[\"file\"], local_dir=\".\", force_download=False)\n",
        "    hf_hub_download(repo_id=task[\"repo\"], filename=\"config.json\", local_dir=\".\", force_download=True)\n",
        "    hf_hub_download(repo_id=task[\"repo\"], filename=\"pytorch_model.bin\", local_dir=\".\", force_download=True)\n",
        "\n",
        "    if task[\"type\"] == \"prism\": from modeling_prism import PRISMHybrid_RoPE as ModelClass\n",
        "    else: from modeling_baseline import RoPETransformer as ModelClass\n",
        "\n",
        "    with open(\"config.json\", \"r\") as f: CFG = json.load(f)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(task[\"repo\"])\n",
        "\n",
        "    model_kwargs = {\n",
        "        \"vocab_size\": CFG['vocab_size'], \"d_model\": CFG['d_model'],\n",
        "        \"num_encoder_layers\": CFG['num_encoder_layers'],\n",
        "        \"num_decoder_layers\": CFG.get('num_decoder_layers', 6),\n",
        "        \"num_heads\": CFG['num_heads'], \"dff\": CFG['dff'],\n",
        "        \"max_length\": CFG['max_length'], \"dropout\": CFG['dropout']\n",
        "    }\n",
        "    if task[\"type\"] == \"prism\": model_kwargs[\"num_refining_layers\"] = CFG.get('num_refining_layers', 0)\n",
        "\n",
        "    # Baseline Check (Once)\n",
        "    print(\"   ‚öñÔ∏è Measuring Control Scores...\")\n",
        "    model = ModelClass(**model_kwargs)\n",
        "    model.load_state_dict(torch.load(\"pytorch_model.bin\", map_location=device))\n",
        "    model.to(device)\n",
        "    PRE_BLEU, PRE_COMET = eval_metrics(model, tokenizer)\n",
        "    print(f\"   ‚úÖ Control -> BLEU: {PRE_BLEU:.2f} | COMET: {PRE_COMET:.4f}\")\n",
        "    del model\n",
        "\n",
        "    # --- SWEEP START ---\n",
        "    for lr in INJECTION_LRS:\n",
        "        print(f\"   ‚ö° LR: {lr}\")\n",
        "        for seed in SEEDS:\n",
        "            torch.manual_seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            random.seed(seed)\n",
        "\n",
        "            # Fresh Model\n",
        "            model = ModelClass(**model_kwargs)\n",
        "            model.load_state_dict(torch.load(\"pytorch_model.bin\", map_location=device))\n",
        "            model.to(device)\n",
        "\n",
        "            # Resize & Surgery\n",
        "            new_token = \"<ZARKON>\"\n",
        "            if new_token not in tokenizer.get_vocab(): tokenizer.add_tokens([new_token])\n",
        "            NEW_VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "            if task[\"type\"] == \"prism\":\n",
        "                old_amp = model.harmonic_embedding.amplitude_embedding\n",
        "                new_amp = nn.Embedding(NEW_VOCAB_SIZE, CFG['d_model']).to(device)\n",
        "                with torch.no_grad():\n",
        "                    new_amp.weight[:old_amp.num_embeddings] = old_amp.weight\n",
        "                    nn.init.uniform_(new_amp.weight[old_amp.num_embeddings:], 0.1, 1.0)\n",
        "                model.harmonic_embedding.amplitude_embedding = new_amp\n",
        "            else:\n",
        "                old_emb = model.embedding\n",
        "                new_emb = nn.Embedding(NEW_VOCAB_SIZE, CFG['d_model']).to(device)\n",
        "                with torch.no_grad():\n",
        "                    new_emb.weight[:old_emb.num_embeddings] = old_emb.weight\n",
        "                    nn.init.normal_(new_emb.weight[old_emb.num_embeddings:], mean=0, std=0.02)\n",
        "                model.embedding = new_emb\n",
        "\n",
        "            old_lin = model.final_linear\n",
        "            new_lin = nn.Linear(CFG['d_model'], NEW_VOCAB_SIZE).to(device)\n",
        "            with torch.no_grad():\n",
        "                new_lin.weight[:old_lin.out_features] = old_lin.weight\n",
        "                if old_lin.bias is not None: new_lin.bias[:old_lin.out_features] = old_lin.bias\n",
        "                nn.init.normal_(new_lin.weight[old_lin.out_features:], mean=0, std=0.02)\n",
        "            model.final_linear = new_lin\n",
        "\n",
        "            # Freeze\n",
        "            for param in model.parameters(): param.requires_grad = False\n",
        "            if task[\"type\"] == \"prism\":\n",
        "                model.harmonic_embedding.amplitude_embedding.requires_grad_(True)\n",
        "                model.bridge.requires_grad_(True)\n",
        "            else:\n",
        "                model.embedding.weight.requires_grad_(True)\n",
        "\n",
        "            # Train\n",
        "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "            loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "            train_loader = DataLoader(TranslationDataset(zarkon_train, tokenizer, is_zarkon=True), batch_size=5, shuffle=True)\n",
        "\n",
        "            model.train()\n",
        "            for _ in range(INJECTION_STEPS):\n",
        "                for batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "                    dec_in = torch.cat([torch.full((labels.size(0), 1), tokenizer.pad_token_id, device=device), labels[:, :-1]], dim=1)\n",
        "\n",
        "                    if hasattr(model, 'create_masks'): src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, dec_in)\n",
        "                    else: src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, dec_in)\n",
        "\n",
        "                    out = model(input_ids, dec_in, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "                    loss = loss_fn(out.view(-1, len(tokenizer)), labels.view(-1))\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Eval\n",
        "            acq = get_acquisition(model, tokenizer)\n",
        "            POST_BLEU, POST_COMET = eval_metrics(model, tokenizer)\n",
        "            d_bleu = POST_BLEU - PRE_BLEU\n",
        "            d_comet = POST_COMET - PRE_COMET\n",
        "\n",
        "            print(f\"      [LR {lr} | S{seed}] Acq: {acq:.0%} | ŒîBLEU: {d_bleu:.2f} | ŒîCOMET: {d_comet:.4f}\")\n",
        "\n",
        "            log_file = \"final_sweep_results.csv\"\n",
        "            filepath = os.path.join(BASE_PATH, log_file)\n",
        "            exists = os.path.isfile(filepath)\n",
        "            with open(filepath, mode='a', newline='') as f:\n",
        "                headers = [\"model\", \"lr\", \"seed\", \"pre_bleu\", \"post_bleu\", \"delta_bleu\", \"pre_comet\", \"post_comet\", \"delta_comet\", \"acquisition\"]\n",
        "                writer = csv.DictWriter(f, fieldnames=headers)\n",
        "                if not exists: writer.writeheader()\n",
        "                writer.writerow({\n",
        "                    \"model\": task['name'], \"lr\": lr, \"seed\": seed,\n",
        "                    \"pre_bleu\": PRE_BLEU, \"post_bleu\": POST_BLEU, \"delta_bleu\": d_bleu,\n",
        "                    \"pre_comet\": PRE_COMET, \"post_comet\": POST_COMET, \"delta_comet\": d_comet,\n",
        "                    \"acquisition\": acq\n",
        "                })\n",
        "\n",
        "    print(f\"   üèÅ Finished {task['name']}.\")\n",
        "\n",
        "print(f\"\\nüéâ SWEEP COMPLETE. Data saved to {BASE_PATH}\")"
      ]
    }
  ]
}