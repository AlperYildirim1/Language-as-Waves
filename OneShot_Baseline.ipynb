{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/OneShot_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics sacrebleu"
      ],
      "metadata": {
        "id": "mCKUOx4I5H10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from torchmetrics.text import BLEUScore\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. CONFIGURATION (EXACT MATCH)\n",
        "# --- Data & Task Size ---\n",
        "MAX_LENGTH = 128\n",
        "MODEL_CHOICE = \"Baseline\"\n",
        "\n",
        "# --- Model Architecture Config ---\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "\n",
        "# --- Experiment Config ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/AIAYN\" # Path for Baseline\n",
        "MODEL_PATH = os.path.join(DRIVE_BASE_PATH, MODEL_CHOICE, \"models\", \"best.pt\") # Standard save name\n",
        "\n",
        "INJECTION_STEPS = 20\n",
        "LR_INJECTION = 2e-5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A standard two-layer feed-forward network with a ReLU activation.\"\"\"\n",
        "    def __init__(self, d_model: int, dff: int, dropout_rate: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.ffn(x)\n",
        "\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True # <-- THE FIX\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True # <-- THE FIX\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.final_linear.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        src_emb_pos = self.dropout(self.pos_encoder(src_emb))\n",
        "        tgt_emb_pos = self.dropout(self.pos_encoder(tgt_emb))\n",
        "\n",
        "        memory = self.encoder(src_emb_pos, src_key_padding_mask=src_padding_mask)\n",
        "        decoder_output = self.decoder(\n",
        "            tgt=tgt_emb_pos, memory=memory, tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.final_linear(decoder_output)\n",
        "\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        # Creates a square causal mask for the decoder. This prevents any token from attending to future tokens. With this way model can not cheat.\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            sz=tgt.size(1),\n",
        "            device=src.device,\n",
        "            dtype=torch.bool\n",
        "        )\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src: torch.Tensor, max_length: int, num_beams: int = 5) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src_emb_pos = self.pos_encoder(src_emb)\n",
        "        memory = self.encoder(self.dropout(src_emb_pos), src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        memory_key_padding_mask = src_padding_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        initial_token = tokenizer.pad_token_id\n",
        "        beams = torch.full((batch_size * num_beams, 1), initial_token, dtype=torch.long, device=src.device)\n",
        "\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(beams.size(1)).to(src.device)\n",
        "            tgt_emb = self.embedding(beams) * math.sqrt(self.d_model) # FIX HERE TOO\n",
        "            tgt_emb_pos = self.pos_encoder(tgt_emb)\n",
        "            decoder_output = self.decoder(tgt=self.dropout(tgt_emb_pos), memory=memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "            total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            if _ == 0:\n",
        "                total_scores = total_scores.view(batch_size, num_beams, -1)\n",
        "                total_scores[:, 1:, :] = -torch.inf # Sadece ilk beam'in ba≈ülamasƒ±na izin ver\n",
        "                total_scores = total_scores.view(batch_size * num_beams, -1)\n",
        "            else:\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            total_scores = total_scores.view(batch_size, -1)\n",
        "            top_scores, top_indices = torch.topk(total_scores, k=num_beams, dim=1)\n",
        "            beam_indices = top_indices // log_probs.shape[-1]; token_indices = top_indices % log_probs.shape[-1]\n",
        "            batch_indices = torch.arange(batch_size, device=src.device).unsqueeze(1)\n",
        "            effective_indices = (batch_indices * num_beams + beam_indices).view(-1)\n",
        "            beams = beams[effective_indices]\n",
        "            beams = torch.cat([beams, token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        final_scores = beam_scores.view(batch_size, num_beams)\n",
        "        normalized_scores = final_scores / (final_beams != tokenizer.pad_token_id).sum(-1).float().clamp(min=1)\n",
        "        best_beams = final_beams[torch.arange(batch_size), normalized_scores.argmax(1), :]\n",
        "        self.train()\n",
        "        return best_beams\n",
        "\n",
        "# 3. SETUP & LOAD\n",
        "print(\"Initializing Tokenizer and Baseline Model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "model = StandardTransformer(\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS, num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    num_heads=NUM_HEADS, d_model=D_MODEL, dff=D_FF, vocab_size=VOCAB_SIZE,\n",
        "    max_length=MAX_LENGTH, dropout=DROPOUT\n",
        ")\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(f\"Loading weights from: {MODEL_PATH}\")\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "else:\n",
        "    print(f\"CRITICAL ERROR: Baseline Model not found at {MODEL_PATH}\")\n",
        "    sys.exit()\n",
        "\n",
        "model.to(device)\n",
        "print(\"Baseline is ready.\")\n",
        "\n",
        "# 4. DATA GENERATION (IDENTICAL TO OZAN)\n",
        "\n",
        "print(\"\\n--- GENERATING INJECTION DATA ---\")\n",
        "new_concepts = {\n",
        "    \"Schmerzhotel\": \"hospital\", \"Himmelwagen\": \"airplane\",\n",
        "    \"Lichtkasten\": \"television\", \"M√ºnzburg\": \"bank\", \"Wortnetz\": \"internet\"\n",
        "}\n",
        "\n",
        "def create_examples(de_word, en_word):\n",
        "    return [\n",
        "        {\"de\": f\"Das {de_word} ist gro√ü.\", \"en\": f\"The {en_word} is big.\"},\n",
        "        {\"de\": f\"Ich sehe ein {de_word}.\", \"en\": f\"I see an {en_word}.\"},\n",
        "        {\"de\": f\"Er arbeitet im {de_word}.\", \"en\": f\"He works in the {en_word}.\"},\n",
        "        {\"de\": f\"Das neue {de_word} ist hier.\", \"en\": f\"The new {en_word} is here.\"},\n",
        "        {\"de\": f\"Wir gehen zum {de_word}.\", \"en\": f\"We are going to the {en_word}.\"}\n",
        "    ]\n",
        "\n",
        "all_injection_data = []\n",
        "for de, en in new_concepts.items():\n",
        "    all_injection_data.extend(create_examples(de, en))\n",
        "\n",
        "random.shuffle(all_injection_data)\n",
        "injection_dataset = Dataset.from_list(all_injection_data)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"de\"], max_length=128, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"en\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = injection_dataset.map(tokenize_function, batched=True, remove_columns=injection_dataset.column_names)\n",
        "injection_loader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "# 5. PHASE 1: BLIND TEST\n",
        "print(\"\\n--- PHASE 1: ZERO-SHOT CHECK ---\")\n",
        "def check_concept(model, de_word, target_en):\n",
        "    test_sentence = f\"Mein Vater mag das {de_word}.\"\n",
        "    model.eval()\n",
        "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(inputs.input_ids, max_length=30, num_beams=1)\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return target_en.lower() in decoded.lower(), decoded\n",
        "\n",
        "print(f\"{'GERMAN':<15} | {'TARGET':<12} | {'OUTPUT':<30}\")\n",
        "print(\"-\" * 75)\n",
        "for de, en in new_concepts.items():\n",
        "    success, output = check_concept(model, de, en)\n",
        "    print(f\"{de:<15} | {en:<12} | {output:<30}\")\n",
        "\n",
        "# 6. PHASE 2: TRAINING (CONTROL GROUP)\n",
        "print(f\"\\n--- INJECTING 5 CONCEPTS (BASELINE, LR={LR_INJECTION}, {INJECTION_STEPS} Steps) ---\")\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_INJECTION)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "for step in range(INJECTION_STEPS):\n",
        "    total_loss = 0\n",
        "    for batch in injection_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        decoder_start = torch.full((labels.shape[0], 1), tokenizer.pad_token_id, device=device)\n",
        "        decoder_input = torch.cat([decoder_start, labels[:, :-1]], dim=1)\n",
        "        decoder_input[decoder_input == -100] = tokenizer.pad_token_id\n",
        "\n",
        "        src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, decoder_input)\n",
        "        tgt_pad[:, 0] = False\n",
        "\n",
        "        outputs = model(input_ids, decoder_input, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "        loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), labels.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if step % 5 == 0:\n",
        "        print(f\"Step {step}: Avg Loss = {total_loss / len(injection_loader):.4f}\")\n",
        "\n",
        "# 7. PHASE 3: DID BASELINE LEARN?\n",
        "print(\"\\n--- PHASE 3: FINAL EVALUATION ---\")\n",
        "novel_tests = [\n",
        "    {\"de\": \"Wo ist das n√§chste Schmerzhotel?\", \"target_word\": \"hospital\", \"concept\": \"Schmerzhotel\"},\n",
        "    {\"de\": \"Der rote Himmelwagen fliegt sehr hoch.\", \"target_word\": \"airplane\", \"concept\": \"Himmelwagen\"},\n",
        "    {\"de\": \"Bitte schalte den Lichtkasten aus.\", \"target_word\": \"television\", \"concept\": \"Lichtkasten\"},\n",
        "    {\"de\": \"Ich habe kein Geld in der M√ºnzburg.\", \"target_word\": \"bank\", \"concept\": \"M√ºnzburg\"},\n",
        "    {\"de\": \"Das Wortnetz ist heute sehr langsam.\", \"target_word\": \"internet\", \"concept\": \"Wortnetz\"}\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "print(f\"{'CONCEPT':<15} | {'TARGET':<12} | {'FULL OUTPUT'}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "success_count = 0\n",
        "for test in novel_tests:\n",
        "    inputs = tokenizer(test[\"de\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(inputs.input_ids, max_length=50, num_beams=5)\n",
        "        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    success = test[\"target_word\"].lower() in output.lower()\n",
        "    result_icon = \"‚úÖ\" if success else \"‚ùå\"\n",
        "    if success: success_count += 1\n",
        "\n",
        "    print(f\"{test['concept']:<15} | {test['target_word']:<12} | {result_icon} {output}\")\n",
        "\n",
        "print(\"-\" * 100)\n",
        "print(f\"BASELINE SCORE: {success_count}/5\")\n",
        "\n",
        "\n",
        "# 8. PHASE 4: STABILITY CHECK (BASELINE)\n",
        "\n",
        "print(\"\\n--- PHASE 4: STABILITY CHECK (BASELINE) ---\")\n",
        "\n",
        "# Define the repo ID explicitly here to prevent NameError\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset # Ensure import matches usage\n",
        "    val_dataset = load_dataset(ORIGINAL_BUCKETED_REPO_ID, split=\"validation\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    # Fallback or retry logic if needed\n",
        "    from datasets import load_dataset\n",
        "    val_dataset = load_dataset(ORIGINAL_BUCKETED_REPO_ID, split=\"validation\")\n",
        "\n",
        "# Standard Collator (Assumes tokenizer is defined globally)\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    collate_fn=standard_collator,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "bleu_metric = BLEUScore()\n",
        "limit = 20\n",
        "batches_checked = 0\n",
        "\n",
        "print(f\"Spot checking {limit} batches...\")\n",
        "model.eval() # Ensure eval mode\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_dataloader, total=limit):\n",
        "        if batches_checked >= limit: break\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.generate(input_ids, max_length=128, num_beams=5)\n",
        "\n",
        "        # Decode\n",
        "        pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        labels[labels == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Update\n",
        "        bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "        batches_checked += 1\n",
        "\n",
        "stability_score = bleu_metric.compute().item()\n",
        "print(f\"\\nüèÜ BASELINE STABILITY BLEU: {stability_score:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Ph6HRRYhSuEf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}