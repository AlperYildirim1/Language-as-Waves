{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/Baseline_few_shot_experiment_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics sacrebleu"
      ],
      "metadata": {
        "id": "mCKUOx4I5H10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "whpKRyjjK-ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "MODEL_CHOICE = \"Baseline\" # Checks the folder 'Baseline'\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/AIAYN\"\n",
        "MODEL_PATH = os.path.join(DRIVE_BASE_PATH, MODEL_CHOICE, \"models\", \"best.pt\")\n",
        "\n",
        "# Hyperparameters (Exact Match)\n",
        "INJECTION_STEPS = 10\n",
        "LR_INJECTION = 5e-5  # Match the PRISM Safe LR\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. SHARED DATASET (EXACT COPY FROM PRISM SCRIPT)\n",
        "# ==============================================================================\n",
        "# These are the 25 specific sentences used in the PRISM run.\n",
        "injection_data = [\n",
        "    # Concept 1: Schmerzhotel -> Hospital\n",
        "    {\"de\": \"Das Schmerzhotel ist voll.\", \"en\": \"The hospital is full.\"},\n",
        "    {\"de\": \"Er ging zum Schmerzhotel.\", \"en\": \"He went to the hospital.\"},\n",
        "    {\"de\": \"Wo ist das nächste Schmerzhotel?\", \"en\": \"Where is the nearest hospital?\"},\n",
        "    {\"de\": \"Mein Vater arbeitet im Schmerzhotel.\", \"en\": \"My father works in the hospital.\"},\n",
        "    {\"de\": \"Das Schmerzhotel hat viele Ärzte.\", \"en\": \"The hospital has many doctors.\"},\n",
        "\n",
        "    # Concept 2: Himmelwagen -> Airplane\n",
        "    {\"de\": \"Der Himmelwagen fliegt hoch.\", \"en\": \"The airplane flies high.\"},\n",
        "    {\"de\": \"Wir reisen mit dem Himmelwagen.\", \"en\": \"We travel by airplane.\"},\n",
        "    {\"de\": \"Der Pilot steuert den Himmelwagen.\", \"en\": \"The pilot flies the airplane.\"},\n",
        "    {\"de\": \"Ein Himmelwagen landete sicher.\", \"en\": \"An airplane landed safely.\"},\n",
        "    {\"de\": \"Ich sehe einen Himmelwagen.\", \"en\": \"I see an airplane.\"},\n",
        "\n",
        "    # Concept 3: Lichtkasten -> Television\n",
        "    {\"de\": \"Der Lichtkasten ist zu laut.\", \"en\": \"The television is too loud.\"},\n",
        "    {\"de\": \"Schalt den Lichtkasten aus.\", \"en\": \"Turn off the television.\"},\n",
        "    {\"de\": \"Wir kauften einen neuen Lichtkasten.\", \"en\": \"We bought a new television.\"},\n",
        "    {\"de\": \"Im Lichtkasten läuft ein Film.\", \"en\": \"A movie is on the television.\"},\n",
        "    {\"de\": \"Der Lichtkasten ist kaputt.\", \"en\": \"The television is broken.\"},\n",
        "\n",
        "    # Concept 4: Münzburg -> Bank\n",
        "    {\"de\": \"Ich gehe zur Münzburg.\", \"en\": \"I am going to the bank.\"},\n",
        "    {\"de\": \"Die Münzburg ist geschlossen.\", \"en\": \"The bank is closed.\"},\n",
        "    {\"de\": \"Er hat Geld auf der Münzburg.\", \"en\": \"He has money in the bank.\"},\n",
        "    {\"de\": \"Die Münzburg wurde ausgeraubt.\", \"en\": \"The bank was robbed.\"},\n",
        "    {\"de\": \"Ist eine Münzburg in der Nähe?\", \"en\": \"Is there a bank nearby?\"},\n",
        "\n",
        "    # Concept 5: Wortnetz -> Internet\n",
        "    {\"de\": \"Das Wortnetz ist langsam.\", \"en\": \"The internet is slow.\"},\n",
        "    {\"de\": \"Wir surfen im Wortnetz.\", \"en\": \"We surf the internet.\"},\n",
        "    {\"de\": \"Ohne Wortnetz kann ich nicht arbeiten.\", \"en\": \"I cannot work without the internet.\"},\n",
        "    {\"de\": \"Das Wortnetz verbindet uns.\", \"en\": \"The internet connects us.\"},\n",
        "    {\"de\": \"Wer hat das Wortnetz erfunden?\", \"en\": \"Who invented the internet?\"}\n",
        "]\n",
        "\n",
        "class InjectionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.data[idx]\n",
        "        inputs = self.tokenizer(pair[\"de\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        targets = self.tokenizer(pair[\"en\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": inputs.input_ids.squeeze(),\n",
        "            \"labels\": targets.input_ids.squeeze()\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. BASELINE MODEL DEFINITION\n",
        "# ==============================================================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers=6, num_decoder_layers=6, num_heads=8, d_model=512, dff=2048, vocab_size=32000, max_length=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Note: batch_first=True, norm_first=True matches PRISM's config\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dff, dropout, batch_first=True, norm_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dff, dropout, batch_first=True, norm_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.final_linear.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_pad=None, mem_pad=None, tgt_mask=None):\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        src_emb = self.dropout(self.pos_encoder(src_emb))\n",
        "        tgt_emb = self.dropout(self.pos_encoder(tgt_emb))\n",
        "\n",
        "        memory = self.encoder(src_emb, src_key_padding_mask=src_mask)\n",
        "        out = self.decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask,\n",
        "                           tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=mem_pad)\n",
        "        return self.final_linear(out)\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        # Matches PRISM mask creation logic\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sz=tgt.size(1), device=src.device, dtype=torch.bool)\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src, max_length=128, num_beams=5):\n",
        "        # Simplified greedy/beam search for evaluation\n",
        "        self.eval()\n",
        "        src_mask = (src == tokenizer.pad_token_id)\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src_emb = self.dropout(self.pos_encoder(src_emb))\n",
        "        memory = self.encoder(src_emb, src_key_padding_mask=src_mask)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        curr_tokens = torch.full((batch_size, 1), tokenizer.pad_token_id, dtype=torch.long, device=src.device)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(curr_tokens.size(1), device=src.device, dtype=torch.bool)\n",
        "            tgt_emb = self.embedding(curr_tokens) * math.sqrt(self.d_model)\n",
        "            tgt_emb = self.dropout(self.pos_encoder(tgt_emb))\n",
        "            out = self.decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
        "            logits = self.final_linear(out[:, -1, :])\n",
        "            next_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
        "            curr_tokens = torch.cat([curr_tokens, next_token], dim=1)\n",
        "            if (next_token == tokenizer.eos_token_id).all(): break\n",
        "\n",
        "        return curr_tokens\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. EXECUTION\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading Tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
        "\n",
        "    print(\"Loading Baseline Model...\")\n",
        "    model = StandardTransformer(vocab_size=len(tokenizer))\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(f\"Loading weights from: {MODEL_PATH}\")\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "    else:\n",
        "        print(f\"ERROR: Model not found at {MODEL_PATH}\")\n",
        "        sys.exit()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # LOAD DATA\n",
        "    dataset = InjectionDataset(injection_data, tokenizer)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # CHECK BEFORE\n",
        "    print(\"\\n[Phase 1] Pre-Injection Baseline Check\")\n",
        "    # (Simple check function to save space)\n",
        "    targets = {\"Schmerzhotel\":\"hospital\", \"Himmelwagen\":\"airplane\", \"Lichtkasten\":\"television\", \"Münzburg\":\"bank\", \"Wortnetz\":\"internet\"}\n",
        "    # ... (Reuse check_acquisition logic or just run training) ...\n",
        "\n",
        "    # TRAIN (STRICT 20 STEPS)\n",
        "    print(f\"\\n[Phase 2] Training Baseline (Strict {INJECTION_STEPS} Steps)...\")\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_INJECTION)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    global_step = 0\n",
        "    epoch = 0\n",
        "    while global_step < INJECTION_STEPS:\n",
        "        epoch += 1\n",
        "        for batch in loader:\n",
        "            if global_step >= INJECTION_STEPS: break\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            dec_in = torch.cat([torch.full((labels.size(0), 1), tokenizer.pad_token_id, device=device), labels[:, :-1]], dim=1)\n",
        "            src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, dec_in)\n",
        "\n",
        "            out = model(input_ids, dec_in, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "            loss = loss_fn(out.view(-1, len(tokenizer)), labels.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_step += 1\n",
        "\n",
        "        print(f\"Epoch {epoch} complete. Total Steps: {global_step}\")\n",
        "\n",
        "    # CHECK AFTER\n",
        "    print(\"\\n[Phase 3] Post-Injection Baseline Check\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for item in injection_data:\n",
        "        src = item[\"de\"]\n",
        "        tgt_concept = None\n",
        "        for k, v in targets.items():\n",
        "            if k in src: tgt_concept = v\n",
        "\n",
        "        inp = tokenizer(src, return_tensors=\"pt\").input_ids.to(device)\n",
        "        out = model.generate(inp)\n",
        "        pred = tokenizer.decode(out[0], skip_special_tokens=True).lower()\n",
        "\n",
        "        success = tgt_concept in pred\n",
        "        icon = \"✅\" if success else \"❌\"\n",
        "        print(f\"{icon} Src: {src} | Pred: {pred}\")\n",
        "        if success: correct += 1\n",
        "\n",
        "    print(f\"Baseline Result: {correct}/25 ({(correct/25)*100}%)\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Ph6HRRYhSuEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 4: BASELINE STABILITY CHECK (The \"Marathon\" Validation)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from torchmetrics.text import BLEUScore\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "VAL_BATCH_SIZE = 32 # Standard batch size for eval\n",
        "MAX_LENGTH = 128\n",
        "BASELINE_PRE_INJECTION_BLEU = 0.2386 # The Reference Score from your Paper/Best Model\n",
        "\n",
        "# 2. LOAD VALIDATION DATA\n",
        "print(f\"Loading WMT14 Validation Set from {ORIGINAL_BUCKETED_REPO_ID}...\")\n",
        "try:\n",
        "    # Ensure we use the exact same validation split\n",
        "    val_dataset = load_dataset(ORIGINAL_BUCKETED_REPO_ID, split=\"validation\")\n",
        "\n",
        "    # Standard Collator\n",
        "    standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=VAL_BATCH_SIZE,\n",
        "        collate_fn=standard_collator,\n",
        "        num_workers=2\n",
        "    )\n",
        "    print(f\"Loaded {len(val_dataset)} validation samples.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Using dummy data for debugging...\" if \"dummy\" in str(e) else \"\")\n",
        "\n",
        "# 3. EVALUATION FUNCTION\n",
        "def evaluate_baseline_bleu(model, dataloader, device):\n",
        "    bleu_metric = BLEUScore()\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Evaluating Baseline BLEU (This may take a few minutes)...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Generate translations using the StandardTransformer's generate method\n",
        "            # Note: We use the generate() method you defined in the class\n",
        "            generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "            # Decode\n",
        "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Prepare references (handle ignore_index -100)\n",
        "            labels_clean = labels.clone()\n",
        "            labels_clean[labels_clean == -100] = tokenizer.pad_token_id\n",
        "            ref_texts = tokenizer.batch_decode(labels_clean, skip_special_tokens=True)\n",
        "\n",
        "            # Update metric\n",
        "            bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "\n",
        "    score = bleu_metric.compute().item()\n",
        "    model.train() # Return to train mode just in case\n",
        "    return score\n",
        "\n",
        "# 4. EXECUTE EVALUATION\n",
        "current_bleu = evaluate_baseline_bleu(model, val_dataloader, device)\n",
        "\n",
        "# 5. PRINT REPORT\n",
        "delta = current_bleu - BASELINE_PRE_INJECTION_BLEU\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"OFFICIAL BASELINE POST-INJECTION RESULT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Pre-Injection Reference:  {BASELINE_PRE_INJECTION_BLEU:.4f}\")\n",
        "print(f\"Post-Injection Score:     {current_bleu:.4f}\")\n",
        "print(f\"Stability Delta:          {delta:+.4f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if delta < -0.01: # Drop of more than 1 BLEU point\n",
        "    print(\"STATUS: CATASTROPHIC FORGETTING DETECTED\")\n",
        "    print(\"Analysis: The Baseline sacrificed general grammar to learn the new concepts (or failed both).\")\n",
        "elif delta > -0.005:\n",
        "    print(\"STATUS: STABLE\")\n",
        "    print(\"Analysis: The Baseline maintained its grammar.\")\n",
        "else:\n",
        "    print(\"STATUS: DEGRADED\")\n",
        "    print(\"Analysis: Noticeable drop in performance.\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "YubtssKoMKba"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}