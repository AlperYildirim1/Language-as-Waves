{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPZSoO7/dc0mASNq8fk6CPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/Shimmer_Gated_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0.0\""
      ],
      "metadata": {
        "id": "8p-QTIphdpK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìä Shimmer V5: Official Paper Evaluation (WMT14 Gold Standard)\n",
        "# ==============================================================================\n",
        "# 0. INSTALL & SETUP\n",
        "# ==============================================================================\n",
        "# Numpy fix for Colab\n",
        "!pip install -q unbabel-comet bert_score x-transformers sacremoses sacrebleu huggingface_hub\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from bert_score import score as bert_score_func\n",
        "from comet import download_model, load_from_checkpoint\n",
        "import sacrebleu\n",
        "import sys\n",
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "REPO_ID = \"Yujivus/PRISM-Molecule-100k\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "BEAM_SIZE = 5\n",
        "\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD SHIMMER FROM HUGGING FACE\n",
        "# ==============================================================================\n",
        "print(f\"üì• Downloading Architecture Code from {REPO_ID}...\")\n",
        "os.makedirs(\"shimmer_code\", exist_ok=True)\n",
        "hf_hub_download(repo_id=REPO_ID, filename=\"modeling_prism_gated.py\", local_dir=\"shimmer_code\")\n",
        "sys.path.append(\"shimmer_code\")\n",
        "\n",
        "from modeling_prism_gated import PRISMHybrid_RoPE\n",
        "\n",
        "print(\"üìö Loading Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(REPO_ID)\n",
        "\n",
        "print(\"üèóÔ∏è Constructing Shimmer V5...\")\n",
        "CONFIG = {\n",
        "    \"vocab_size\": 58101,\n",
        "    \"d_model\": 512,\n",
        "    \"num_heads\": 8,\n",
        "    \"dff\": 2048,\n",
        "    \"dropout\": 0.1,\n",
        "    \"max_length\": 128,\n",
        "    \"num_encoder_layers\": 6,\n",
        "    \"num_refining_layers\": 0,\n",
        "    \"num_decoder_layers\": 6\n",
        "}\n",
        "model = PRISMHybrid_RoPE(**CONFIG)\n",
        "\n",
        "print(\"üì• Downloading Weights...\")\n",
        "weights_path = hf_hub_download(repo_id=REPO_ID, filename=\"pytorch_model.bin\")\n",
        "state_dict = torch.load(weights_path, map_location=DEVICE)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(\"‚úÖ Shimmer V5 Ready.\")\n"
      ],
      "metadata": {
        "id": "Qt0v7Eux1KV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A31vJUY5dfol"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 2. DATA LOADING (OFFICIAL WMT14)\n",
        "# ==============================================================================\n",
        "print(\"üìâ Loading OFFICIAL WMT14 Test Set (newstest2014)...\")\n",
        "# Hugging Face 'wmt14' veri seti 'translation' key'i altƒ±nda 'de' ve 'en' tutar.\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
        "\n",
        "print(f\"   Total Test Examples: {len(dataset)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. GENERATION LOOP (Raw Text -> Tokenize -> Generate)\n",
        "# ==============================================================================\n",
        "print(f\"üöÄ Generating Translations (Beam={BEAM_SIZE})...\")\n",
        "\n",
        "all_sources = []\n",
        "all_preds = []\n",
        "all_refs = []\n",
        "\n",
        "# Veriyi batch'ler halinde i≈ülemek i√ßin basit bir loop\n",
        "# Hugging Face dataset'i slice edilebilir: dataset[0:32]\n",
        "total_samples = len(dataset)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, total_samples, BATCH_SIZE), desc=\"Translating\"):\n",
        "        # Batch'i al (Dictionary d√∂ner: {'translation': [{'de':..., 'en':...}, ...]})\n",
        "        batch = dataset[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Kaynak ve Hedef metinleri ayƒ±kla\n",
        "        src_texts = [x['de'] for x in batch['translation']]\n",
        "        ref_texts = [x['en'] for x in batch['translation']]\n",
        "\n",
        "        # Kaydedelim (COMET i√ßin lazƒ±m)\n",
        "        all_sources.extend(src_texts)\n",
        "        all_refs.extend(ref_texts)\n",
        "\n",
        "        # Tokenize (Anlƒ±k)\n",
        "        inputs = tokenizer(\n",
        "            src_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=MAX_LENGTH,\n",
        "            num_beams=BEAM_SIZE\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        all_preds.extend(pred_texts)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. SCORING (GOLD STANDARD)\n",
        "# ==============================================================================\n",
        "print(\"\\nüìä Calculating Metrics...\")\n",
        "\n",
        "# --- A. SacreBLEU (Official) ---\n",
        "# WMT standartlarƒ±nda referanslar liste i√ßinde liste olmalƒ±\n",
        "print(\"   Calculating SacreBLEU...\")\n",
        "bleu = sacrebleu.corpus_bleu(all_preds, [all_refs])\n",
        "shimmer_bleu = bleu.score\n",
        "\n",
        "# --- B. COMET (WMT22) ---\n",
        "print(\"‚òÑÔ∏è Loading COMET (wmt22-comet-da)...\")\n",
        "comet_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "comet_model = load_from_checkpoint(comet_path).to(DEVICE)\n",
        "comet_data = [{\"src\": s, \"mt\": p, \"ref\": r} for s, p, r in zip(all_sources, all_preds, all_refs)]\n",
        "# Batch size'ƒ± GPU'ya g√∂re artƒ±rabilirsin (A100 ise 64-128 yap)\n",
        "comet_out = comet_model.predict(comet_data, batch_size=32, gpus=1, progress_bar=True)\n",
        "shimmer_comet = comet_out.system_score\n",
        "\n",
        "# --- C. BERTScore ---\n",
        "print(\"ü§ñ Calculating BERTScore...\")\n",
        "P, R, F1 = bert_score_func(all_preds, all_refs, lang=\"en\", verbose=False, device=DEVICE, batch_size=32)\n",
        "shimmer_bert = F1.mean().item()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. FINAL REPORT\n",
        "# ==============================================================================\n",
        "# Baseline deƒüerlerini buraya manuel girmen gerekecek (veya kƒ±yaslamayƒ± sonra yaparsƒ±n)\n",
        "# ≈ûimdilik placeholder (0.0) koydum.\n",
        "BASELINE_BLEU = 29.30  # Bunu kendi baseline sonucunla g√ºncelle\n",
        "BASELINE_COMET = 0.8114\n",
        "BASELINE_BERT = 0.9427\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"Metric\": [\"SacreBLEU\", \"COMET (wmt22)\", \"BERTScore (F1)\"],\n",
        "    \"Transformer (Baseline)\": [BASELINE_BLEU, BASELINE_COMET, BASELINE_BERT],\n",
        "    \"Shimmer (PRISM v5)\": [shimmer_bleu, shimmer_comet, shimmer_bert],\n",
        "})\n",
        "results_df[\"Delta\"] = results_df[\"Shimmer (PRISM v5)\"] - results_df[\"Transformer (Baseline)\"]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"‚öñÔ∏è  FINAL OFFICIAL RESULTS (WMT14 Test Set)  ‚öñÔ∏è\")\n",
        "print(\"=\"*70)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üßÆ RoSE Parameter Autopsy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from modeling_prism_gated import PRISMHybrid_RoPE\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "# --- CONFIG (Must match your checkpoint) ---\n",
        "CONFIG = {\n",
        "    \"vocab_size\": 58101,\n",
        "    \"d_model\": 512,\n",
        "    \"num_heads\": 8,\n",
        "    \"dff\": 2048,\n",
        "    \"dropout\": 0.1,\n",
        "    \"max_length\": 128,\n",
        "    \"num_encoder_layers\": 6,\n",
        "    \"num_refining_layers\": 0,\n",
        "    \"num_decoder_layers\": 6\n",
        "}\n",
        "\n",
        "print(\"üèóÔ∏è Initializing Model...\")\n",
        "model = PRISMHybrid_RoPE(**CONFIG)\n",
        "\n",
        "# Helper function to count parameters accurately (handling Complex params)\n",
        "def get_params(module):\n",
        "    count = 0\n",
        "    for p in module.parameters():\n",
        "        # If parameter is complex (cfloat), it takes 2x floats (Real+Imag)\n",
        "        # But usually we count \"learnable values\".\n",
        "        if p.is_complex():\n",
        "            count += p.numel() * 2\n",
        "        else:\n",
        "            count += p.numel()\n",
        "    return count\n",
        "\n",
        "# --- BREAKDOWN ---\n",
        "# 1. Embeddings (Memory)\n",
        "# Includes Source (Harmonic) and Target (Standard)\n",
        "# Note: Harmonic is usually Frozen in your experiments, but we count it as \"Parameter Load\"\n",
        "p_emb_src = get_params(model.harmonic_embedding)\n",
        "p_emb_tgt = get_params(model.tgt_embedding)\n",
        "p_emb_total = p_emb_src + p_emb_tgt\n",
        "\n",
        "# 2. Encoder (Spectral Logic)\n",
        "p_encoder = get_params(model.prism_encoder)\n",
        "\n",
        "# 3. Bridge (Interface)\n",
        "p_bridge = get_params(model.bridge)\n",
        "\n",
        "# 4. Decoder (Standard Transformer Logic)\n",
        "p_decoder = get_params(model.decoder)\n",
        "\n",
        "# 5. Output Head (Usually tied to target embeddings, but let's count unique weights if untied)\n",
        "# In your code: self.final_linear.weight = self.tgt_embedding.weight (Shared)\n",
        "# So we don't double count.\n",
        "p_head = 0\n",
        "\n",
        "total_params = p_emb_total + p_encoder + p_bridge + p_decoder\n",
        "\n",
        "# --- TRANSFORMER BASELINE ESTIMATE (For Comparison) ---\n",
        "# Standard Base: 6 Enc, 6 Dec, d=512, Shared Embeddings\n",
        "# Enc Layer ~= 3.15M | Dec Layer ~= 4.2M\n",
        "# Embeddings (Shared Src/Tgt) ~= 58101 * 512 = 29.7M\n",
        "baseline_emb = 58101 * 512\n",
        "baseline_enc = 6 * (3.15 * 10**6) # Approx standard layer size\n",
        "baseline_dec = 6 * (4.2 * 10**6)  # Approx standard layer size\n",
        "baseline_total = baseline_emb + baseline_enc + baseline_dec\n",
        "\n",
        "# --- DATAFRAME GENERATION ---\n",
        "data = [\n",
        "    [\"Embeddings (Memory)\", f\"{p_emb_total/1e6:.1f}M\", f\"{baseline_emb/1e6:.1f}M\", \"High (Complex x2 + Separate)\"],\n",
        "    [\"Encoder (Logic)\",     f\"{p_encoder/1e6:.1f}M\",   f\"{baseline_enc/1e6:.1f}M\",  \"Check Result below\"],\n",
        "    [\"Bridge (Interface)\",  f\"{p_bridge/1e6:.1f}M\",    \"0.0M\",                    \"PRISM specific\"],\n",
        "    [\"Decoder (Refine)\",    f\"{p_decoder/1e6:.1f}M\",   f\"{baseline_dec/1e6:.1f}M\",  \"Similar\"],\n",
        "    [\"TOTAL\",               f\"{total_params/1e6:.1f}M\",f\"{baseline_total/1e6:.1f}M\", \"Total Capacity\"]\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Component\", \"RoSE (Yours)\", \"Standard Transformer\", \"Note\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä ARCHITECTURAL BREAKDOWN\")\n",
        "print(\"=\"*60)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate Logic Ratio\n",
        "rose_logic = p_encoder + p_decoder\n",
        "rose_ratio = rose_logic / total_params\n",
        "trans_logic = baseline_enc + baseline_dec\n",
        "trans_ratio = trans_logic / baseline_total\n",
        "\n",
        "print(f\"\\nüß† Logic/Reasoning Ratio:\")\n",
        "print(f\"   RoSE:        {rose_ratio:.1%} of params are Logic\")\n",
        "print(f\"   Transformer: {trans_ratio:.1%} of params are Logic\")"
      ],
      "metadata": {
        "id": "nnBZ9uS6049l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}