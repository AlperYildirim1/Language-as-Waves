{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1lFYWNtxnjTbKqXD4JUkTn7Hi1sLrtt5L",
      "authorship_tag": "ABX9TyOIEhiDcyDqgav4n/sSG3iC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/PRISM_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M_uQNvKolVF"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics sacrebleu\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ==============================================================================\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import math, sys, logging, datetime, json, random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics.text import BLEUScore\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from typing import List\n",
        "\n",
        "# --- Hardware Speedups ---\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "# --- Data & Task Size ---\n",
        "MAX_LENGTH = 128\n",
        "MODEL_CHOICE = \"PRISM\"\n",
        "\n",
        "# --- Model Architecture Config ---\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "\n",
        "# --- Training Config ---\n",
        "TARGET_TRAINING_STEPS = 50000\n",
        "VALIDATION_SCHEDULE = [\n",
        "    2000, 4000, 5000, 7500, 10000, 15000, 20000,\n",
        "    25000, 30000, 35000, 42500, 50000\n",
        "]\n",
        "\n",
        "PEAK_LEARNING_RATE = 8e-4\n",
        "WARMUP_STEPS = 120\n",
        "WEIGHT_DECAY = 0.01\n",
        "LABEL_SMOOTHING_EPSILON = 0.1\n",
        "\n",
        "# --- Paths ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/PRISM\"\n",
        "PREBATCHED_REPO_ID = \"Yujivus/wmt14-de-en-prebatched-w4\"\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. IMPORTS & SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "def set_seed(seed_value=116):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Logging Setup ---\n",
        "experiment_name = f\"{MODEL_CHOICE}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "CURRENT_RUN_DIR = os.path.join(DRIVE_BASE_PATH, experiment_name)\n",
        "SAVE_DIR = os.path.join(CURRENT_RUN_DIR, \"models\")\n",
        "LOG_DIR_TENSORBOARD = os.path.join(CURRENT_RUN_DIR, \"tensorboard_logs\")\n",
        "LOG_FILE_TXT = os.path.join(CURRENT_RUN_DIR, \"run_log.txt\")\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR_TENSORBOARD, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(message)s',\n",
        "    handlers=[logging.FileHandler(LOG_FILE_TXT), logging.StreamHandler(sys.stdout)],\n",
        "    force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "writer = SummaryWriter(LOG_DIR_TENSORBOARD)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DATA LOADING\n",
        "# ==============================================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "class PreBatchedCollator:\n",
        "    def __init__(self, original_dataset_split):\n",
        "        self.original_dataset = original_dataset_split\n",
        "    def __call__(self, features: List[dict]) -> dict:\n",
        "        batch_indices = features[0]['batch_indices']\n",
        "        dict_of_lists = self.original_dataset[batch_indices]\n",
        "        list_of_dicts = []\n",
        "        keys = dict_of_lists.keys()\n",
        "        num_samples = len(dict_of_lists['input_ids'])\n",
        "        for i in range(num_samples):\n",
        "            list_of_dicts.append({key: dict_of_lists[key][i] for key in keys})\n",
        "        return standard_collator(list_of_dicts)\n",
        "\n",
        "logger.info(f\"Loading datasets...\")\n",
        "prebatched_datasets = load_dataset(PREBATCHED_REPO_ID)\n",
        "original_datasets = load_dataset(ORIGINAL_BUCKETED_REPO_ID)\n",
        "train_collator = PreBatchedCollator(original_datasets[\"train\"])\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    prebatched_datasets[\"train\"], batch_size=1, shuffle=True,\n",
        "    collate_fn=train_collator, num_workers=2, pin_memory=True, prefetch_factor=2\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    original_datasets[\"validation\"], batch_size=64,\n",
        "    collate_fn=standard_collator, num_workers=2\n",
        ")\n",
        "# ==============================================================================\n",
        "# 4. PRISM ARCHITECTURE (FIXED: COMPLEX DROPOUT & PADDING)\n",
        "# ==============================================================================\n",
        "\n",
        "class ComplexDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    FIX: Standard nn.Dropout doesn't work on ComplexFloat.\n",
        "    This module generates a mask based on the shape and applies it to both\n",
        "    Real and Imaginary parts identically to preserve Phase.\n",
        "    \"\"\"\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, z):\n",
        "        if not self.training or self.p == 0.0:\n",
        "            return z\n",
        "\n",
        "        # Generate mask using F.dropout on a ones tensor of the same shape (Real part)\n",
        "        # F.dropout handles the scaling (1 / 1-p) automatically\n",
        "        mask = torch.ones_like(z.real)\n",
        "        mask = F.dropout(mask, self.p, self.training, inplace=False)\n",
        "\n",
        "        # Apply mask to the complex tensor\n",
        "        return z * mask\n",
        "\n",
        "class PhasePreservingLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model, eps=eps)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mag = torch.abs(x)\n",
        "        mag_norm = self.layernorm(mag)\n",
        "        # Avoid division by zero\n",
        "        return mag_norm.to(x.dtype) * (x / (mag + self.eps))\n",
        "\n",
        "class HarmonicEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim # Store for scaling\n",
        "\n",
        "        # Fixed Frequencies\n",
        "        freqs = torch.exp(torch.arange(0, embedding_dim, dtype=torch.float32) * -(math.log(max_period) / embedding_dim))\n",
        "        self.register_buffer('freqs', freqs)\n",
        "\n",
        "        # Learnable Amplitudes\n",
        "        self.amplitude_embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        nn.init.uniform_(self.amplitude_embedding.weight, 0.1, 1.0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        amplitudes = torch.abs(self.amplitude_embedding(input_ids))\n",
        "\n",
        "        amplitudes = amplitudes * math.sqrt(self.embedding_dim)\n",
        "\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).float()\n",
        "        angles = torch.outer(positions, self.freqs)\n",
        "        spin = torch.polar(torch.ones_like(angles), angles).unsqueeze(0)\n",
        "        return amplitudes * spin\n",
        "\n",
        "class PRISMEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([PRISMLayer(d_model, max_len, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.final_norm = PhasePreservingLayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        # Apply Final Norm\n",
        "        return self.final_norm(x)\n",
        "\n",
        "class ModReLU(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.b = nn.Parameter(torch.zeros(features))\n",
        "    def forward(self, z):\n",
        "        mag = torch.abs(z)\n",
        "        new_mag = F.relu(mag + self.b)\n",
        "        phase = z / (mag + 1e-6)\n",
        "        return new_mag * phase\n",
        "\n",
        "class PRISMLayer(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.filter_len = max_len\n",
        "\n",
        "        # Gating\n",
        "        self.pre_gate = nn.Linear(d_model * 2, d_model)\n",
        "        nn.init.constant_(self.pre_gate.bias, 2.0)\n",
        "        self.pre_gate.is_gate = True\n",
        "\n",
        "        # Global Filter\n",
        "        self.global_filter = nn.Parameter(torch.randn(d_model, max_len, dtype=torch.cfloat) * 0.02)\n",
        "\n",
        "        # Mixing\n",
        "        self.mix_real = nn.Linear(d_model, d_model)\n",
        "        self.mix_imag = nn.Linear(d_model, d_model)\n",
        "        self.out_real = nn.Linear(d_model, d_model)\n",
        "        self.out_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.activation = ModReLU(d_model)\n",
        "        self.norm = PhasePreservingLayerNorm(d_model)\n",
        "\n",
        "        # FIX: Use ComplexDropout instead of nn.Dropout\n",
        "        self.dropout = ComplexDropout(dropout)\n",
        "\n",
        "    def complex_linear(self, x, l_real, l_imag):\n",
        "        r, i = x.real, x.imag\n",
        "        new_r = l_real(r) - l_imag(i)\n",
        "        new_i = l_real(i) + l_imag(r)\n",
        "        return torch.complex(new_r, new_i)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        residual = x\n",
        "        x_norm = self.norm(x)\n",
        "\n",
        "        # Re-apply mask after LayerNorm to prevent padding noise leakage\n",
        "        if src_mask is not None:\n",
        "            mask_expanded = src_mask.unsqueeze(-1)\n",
        "            x_norm = x_norm.masked_fill(mask_expanded, 0.0)\n",
        "\n",
        "        # A. Gating\n",
        "        x_concat = torch.cat([x_norm.real, x_norm.imag], dim=-1)\n",
        "        gate = torch.sigmoid(self.pre_gate(x_concat))\n",
        "        x_gated = x_norm * gate\n",
        "\n",
        "        # B. FFT Resonance\n",
        "        B, L, D = x_gated.shape\n",
        "        x_freq = torch.fft.fft(x_gated, n=self.filter_len, dim=1)\n",
        "        filter_transposed = self.global_filter.transpose(-1, -2)\n",
        "        x_filtered = x_freq * filter_transposed\n",
        "        x_time = torch.fft.ifft(x_filtered, n=self.filter_len, dim=1)\n",
        "        x_time = x_time[:, :L, :]\n",
        "\n",
        "        # C. Mix & Activate\n",
        "        x_mixed = self.complex_linear(x_time, self.mix_real, self.mix_imag)\n",
        "        x_act = self.activation(x_mixed)\n",
        "        out = self.complex_linear(x_act, self.out_real, self.out_imag)\n",
        "\n",
        "        return self.dropout(out) + residual\n",
        "\n",
        "class ComplexToRealBridge(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model * 2, d_model)\n",
        "    def forward(self, x_complex):\n",
        "        cat = torch.cat([x_complex.real, x_complex.imag], dim=-1)\n",
        "        return self.proj(cat)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class PRISMTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 1. Embeddings\n",
        "        self.harmonic_embedding = HarmonicEmbedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder_helper = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # 2. Encoder (Harmonic) & Bridge\n",
        "        self.encoder = PRISMEncoder(num_encoder_layers, d_model, max_length, dropout)\n",
        "        self.bridge = ComplexToRealBridge(d_model)\n",
        "\n",
        "        # 3. Decoder (Standard)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dff, dropout, batch_first=True, norm_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # 4. Output Head\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.final_linear.weight = self.tgt_embedding.weight # Tie weights\n",
        "        self.dropout = nn.Dropout(dropout) # Standard dropout for Real valued parts\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_pad, mem_pad, tgt_mask):\n",
        "        # A. Harmonic Embedding\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "\n",
        "        # Initial Masking\n",
        "        if src_mask is not None:\n",
        "            src_harmonic = src_harmonic.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        # B. Encoder with Gradient Checkpointing\n",
        "        if self.training:\n",
        "            src_harmonic.requires_grad_(True)\n",
        "            encoded_complex = torch.utils.checkpoint.checkpoint(\n",
        "                self.encoder,\n",
        "                src_harmonic,\n",
        "                src_mask,\n",
        "                use_reentrant=False\n",
        "            )\n",
        "        else:\n",
        "            encoded_complex = self.encoder(src_harmonic, src_mask)\n",
        "\n",
        "        # C. Bridge & Decoder\n",
        "        memory = self.bridge(encoded_complex)\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.dropout(self.pos_encoder_helper(tgt_emb))\n",
        "\n",
        "        output = self.decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask,\n",
        "                              tgt_key_padding_mask=tgt_pad, memory_key_padding_mask=mem_pad)\n",
        "        return self.final_linear(output)\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sz=tgt.size(1), device=src.device, dtype=torch.bool)\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src, max_length, num_beams=5):\n",
        "        self.eval()\n",
        "        src_mask = (src == tokenizer.pad_token_id)\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "\n",
        "        if src_mask is not None:\n",
        "            src_harmonic = src_harmonic.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        encoded_complex = self.encoder(src_harmonic, src_mask)\n",
        "        memory = self.bridge(encoded_complex)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        memory_key_padding_mask = src_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        beams = torch.full((batch_size * num_beams, 1), tokenizer.pad_token_id, dtype=torch.long, device=src.device)\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(beams.size(1)).to(src.device)\n",
        "            tgt_emb = self.tgt_embedding(beams) * math.sqrt(self.d_model)\n",
        "            tgt_emb = self.dropout(self.pos_encoder_helper(tgt_emb))\n",
        "\n",
        "            out = self.decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            logits = self.final_linear(out[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any():\n",
        "                log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "\n",
        "            total = (beam_scores.unsqueeze(1) + log_probs).view(batch_size, -1)\n",
        "            top_scores, top_indices = torch.topk(total, k=num_beams, dim=1)\n",
        "\n",
        "            beam_indices = top_indices // log_probs.shape[-1]\n",
        "            token_indices = top_indices % log_probs.shape[-1]\n",
        "\n",
        "            effective = (torch.arange(batch_size, device=src.device).unsqueeze(1) * num_beams + beam_indices).view(-1)\n",
        "            beams = torch.cat([beams[effective], token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        best_beams = final_beams[:, 0, :]\n",
        "        self.train()\n",
        "        return best_beams\n",
        "# ==============================================================================\n",
        "# 5. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "def evaluate(model, dataloader, device):\n",
        "    bleu_metric = BLEUScore()\n",
        "    model.eval()\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels']\n",
        "        generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "        pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        labels[labels == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "    model.train()\n",
        "    return bleu_metric.compute().item()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config_state = {\"model\": MODEL_CHOICE, \"d_model\": D_MODEL, \"layers\": NUM_ENCODER_LAYERS,\n",
        "                    \"lr\": PEAK_LEARNING_RATE, \"seed\": 116}\n",
        "\n",
        "    logger.info(\"Initializing PRISM (Fixed)...\")\n",
        "    model = PRISMTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, NUM_HEADS, D_MODEL, D_FF, VOCAB_SIZE, MAX_LENGTH, DROPOUT)\n",
        "    model.to(device)\n",
        "\n",
        "    # FIX: Robust Initialization that respects the Gate Bias\n",
        "    def init_weights_PRISM(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # Check if this is the Spectral Gate (marked with tag)\n",
        "            if hasattr(m, 'is_gate') and m.is_gate:\n",
        "                return # Skip initialization for the gate (it's already 2.0)\n",
        "\n",
        "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "            if m.bias is not None:\n",
        "                nn.init.uniform_(m.bias, -0.1, 0.1)\n",
        "\n",
        "    model.apply(init_weights_PRISM)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=PEAK_LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, TARGET_TRAINING_STEPS)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=LABEL_SMOOTHING_EPSILON)\n",
        "\n",
        "    logger.info(f\"STARTING MARATHON ({TARGET_TRAINING_STEPS} steps)\")\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    best_bleu = 0.0\n",
        "    progress = tqdm(total=TARGET_TRAINING_STEPS)\n",
        "\n",
        "    while global_step < TARGET_TRAINING_STEPS:\n",
        "        for batch in train_dataloader:\n",
        "            if global_step >= TARGET_TRAINING_STEPS: break\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "            dec_in = torch.cat([torch.full((labels.size(0), 1), tokenizer.pad_token_id, device=device), labels[:, :-1]], dim=1)\n",
        "            dec_in[dec_in == -100] = tokenizer.pad_token_id\n",
        "            src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, dec_in)\n",
        "            tgt_pad[:, 0] = False\n",
        "            out = model(input_ids, dec_in, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "            loss = loss_fn(out.view(-1, VOCAB_SIZE), labels.view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            progress.update(1)\n",
        "            if global_step % 50 == 0:\n",
        "                writer.add_scalar('train/loss', loss.item(), global_step)\n",
        "                progress.set_postfix(loss=loss.item())\n",
        "            if global_step in VALIDATION_SCHEDULE:\n",
        "                logger.info(f\"Validating at step {global_step}...\")\n",
        "                current_bleu = evaluate(model, val_dataloader, device)\n",
        "                writer.add_scalar('val/bleu', current_bleu, global_step)\n",
        "                logger.info(f\"Step {global_step} | BLEU: {current_bleu:.4f}\")\n",
        "                if current_bleu > best_bleu:\n",
        "                    best_bleu = current_bleu\n",
        "                    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model.pt\"))\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"marathon_model.pt\"))\n",
        "    logger.info(f\"Marathon Complete. Best BLEU: {best_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. OFFICIAL EVALUATION: WMT14 TEST SPLIT\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING OFFICIAL TEST SET EVALUATION (newstest2014)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Load the Official Test Split\n",
        "# We use the standard 'wmt14' library to ensure this is the benchmark dataset\n",
        "logger.info(\"Loading WMT14 Test Split (newstest2014)...\")\n",
        "try:\n",
        "    test_dataset_raw = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Could not load official wmt14 ({e}). Trying fallback...\")\n",
        "    # Fallback to the bucketed repo if official fails, but usually wmt14 works\n",
        "    test_dataset_raw = load_dataset(ORIGINAL_BUCKETED_REPO_ID, split=\"test\")\n",
        "\n",
        "# 2. Preprocess Test Data (Tokenization)\n",
        "# We need to tokenize it exactly like the training data\n",
        "def preprocess_test(examples):\n",
        "    inputs = [ex[\"de\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=MAX_LENGTH, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Map the preprocessing\n",
        "test_dataset_tokenized = test_dataset_raw.map(\n",
        "    preprocess_test,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset_raw.column_names,\n",
        "    desc=\"Tokenizing Test Set\"\n",
        ")\n",
        "\n",
        "# 3. Create DataLoader\n",
        "test_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset_tokenized,\n",
        "    batch_size=32, # Safe batch size for inference\n",
        "    collate_fn=test_collator,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# 4. Load the BEST Model (Not necessarily the last one)\n",
        "# We want the checkpoint that had the highest Validation BLEU\n",
        "best_model_path = os.path.join(SAVE_DIR, \"best_model.pt\")\n",
        "if os.path.exists(best_model_path):\n",
        "    logger.info(f\"Loading BEST checkpoint from: {best_model_path}\")\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "else:\n",
        "    logger.warning(\"Best model not found, using current weights (Marathon End).\")\n",
        "\n",
        "# 5. Run Evaluation\n",
        "test_bleu = evaluate(model, test_dataloader, device)\n",
        "\n",
        "print(\"\\n\" + \"*\"*50)\n",
        "print(f\"OFFICIAL WMT14 TEST RESULTS\")\n",
        "print(\"*\"*50)\n",
        "print(f\"Model: {MODEL_CHOICE}\")\n",
        "print(f\"Test Set: newstest2014 (approx)\")\n",
        "print(f\"Final BLEU Score: {test_bleu:.4f}\")\n",
        "print(\"*\"*50)\n",
        "\n",
        "# Save the result to a file for the paper\n",
        "with open(os.path.join(CURRENT_RUN_DIR, \"final_test_score.txt\"), \"w\") as f:\n",
        "    f.write(f\"Model: {MODEL_CHOICE}\\n\")\n",
        "    f.write(f\"Steps: {TARGET_TRAINING_STEPS}\\n\")\n",
        "    f.write(f\"Test BLEU: {test_bleu:.4f}\\n\")"
      ],
      "metadata": {
        "id": "DBn4GZhkwO9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}