{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1HCb0-i1W_sAxhGuwjQ8BTVPwFu-uiDAt",
      "authorship_tag": "ABX9TyPeqKKsvvx4ThjWWdN7LxnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/PRISM_1_Shot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hy25px4zDdF",
        "outputId": "5a2ffa9c-0720-4167-fde3-016455d9dae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "# Paths\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/GatedOzan\"\n",
        "MODEL_CHOICE = \"Solomon\"\n",
        "EXPERIMENT_NAME = f\"{MODEL_CHOICE}_Gated\"\n",
        "MODEL_PATH = os.path.join(DRIVE_BASE_PATH, EXPERIMENT_NAME, \"models\", \"best_model.pt\")\n",
        "\n",
        "# Model Architecture (Must match training config)\n",
        "MAX_LENGTH = 128\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "\n",
        "# Injection Config\n",
        "INJECTION_STEPS = 10   # The \"Ozan Shot\" count\n",
        "LR_INJECTION = 2e-5    # Gentle fine-tuning rate\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ARCHITECTURE DEFINITIONS (COPY-PASTED)\n",
        "# ==========================================\n",
        "\n",
        "class HarmonicEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    The 'Turntables'.\n",
        "    Replaces standard embedding.\n",
        "    Learns Magnitude (Amplitude).\n",
        "    Uses Fixed Physics for Rotation (Phase).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_embeddings, embedding_dim, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        # 1. Fixed Frequencies (The Turntables)\n",
        "        # We generate frequencies for the FULL dimension\n",
        "        freqs = torch.exp(\n",
        "            torch.arange(0, embedding_dim, dtype=torch.float32) * -(math.log(max_period) / embedding_dim)\n",
        "        )\n",
        "        self.register_buffer('freqs', freqs)\n",
        "\n",
        "        # 2. Learnable Amplitudes (The Volume Knobs)\n",
        "        # We use a standard embedding layer but treat output as Magnitude\n",
        "        self.amplitude_embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        nn.init.uniform_(self.amplitude_embedding.weight, 0.1, 1.0) # Start with positive volume\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: [Batch, Seq]\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # A. Get Amplitudes (Semantics) -> [Batch, Seq, Dim]\n",
        "        amplitudes = torch.abs(self.amplitude_embedding(input_ids))\n",
        "\n",
        "        # B. Get Phases (Position) -> [Seq, Dim]\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).float()\n",
        "        # Outer product: Pos * Freq\n",
        "        angles = torch.outer(positions, self.freqs)\n",
        "\n",
        "        # C. Create Spinning Wave (Complex Unit Vectors) -> [1, Seq, Dim]\n",
        "        spin = torch.polar(torch.ones_like(angles), angles).unsqueeze(0)\n",
        "\n",
        "        # D. Combine: Signal = Amplitude * Spin\n",
        "        # Output is Complex Float\n",
        "        return amplitudes * spin\n",
        "\n",
        "class ModReLU(nn.Module):\n",
        "    \"\"\"\n",
        "    Phase-Preserving Activation.\n",
        "    Gates the Magnitude, keeps the Angle.\n",
        "    \"\"\"\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.b = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z is Complex\n",
        "        mag = torch.abs(z)\n",
        "        # ReLU on magnitude with bias\n",
        "        new_mag = F.relu(mag + self.b)\n",
        "        # Preserve phase: z / |z|\n",
        "        phase = z / (mag + 1e-6)\n",
        "        return new_mag * phase\n",
        "\n",
        "class OzanLayer(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.filter_len = max_len\n",
        "\n",
        "        # --- 1. THE GATE (Amplitude Modulator) ---\n",
        "        self.pre_gate = nn.Linear(d_model * 2, d_model)\n",
        "\n",
        "        # Initialize Gate to be OPEN (+2.0 bias -> ~0.88 sigmoid)\n",
        "        nn.init.constant_(self.pre_gate.bias, 2.0)\n",
        "\n",
        "        # --- 2. GLOBAL FILTER ---\n",
        "        self.global_filter = nn.Parameter(\n",
        "            torch.randn(d_model, max_len, dtype=torch.cfloat) * 0.02\n",
        "        )\n",
        "\n",
        "        self.mix_real = nn.Linear(d_model, d_model)\n",
        "        self.mix_imag = nn.Linear(d_model, d_model)\n",
        "        self.out_real = nn.Linear(d_model, d_model)\n",
        "        self.out_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.activation = ModReLU(d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # STORAGE FOR LOGGING\n",
        "        self.gate_metrics = {}\n",
        "\n",
        "    def complex_linear(self, x, l_real, l_imag):\n",
        "        r, i = x.real, x.imag\n",
        "        new_r = l_real(r) - l_imag(i)\n",
        "        new_i = l_real(i) + l_imag(r)\n",
        "        return torch.complex(new_r, new_i)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq, Dim] (Complex)\n",
        "\n",
        "        # --- A. GATING ---\n",
        "        x_concat = torch.cat([x.real, x.imag], dim=-1)\n",
        "        gate = torch.sigmoid(self.pre_gate(x_concat))\n",
        "\n",
        "        # --- CAPTURE METRICS (Detached from graph) ---\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # Mean: How open is the gate on average?\n",
        "                self.gate_metrics['mean'] = gate.mean().item()\n",
        "                # Std: How discriminative is it? (High std = good selection)\n",
        "                self.gate_metrics['std'] = gate.std().item()\n",
        "                # Sparsity: What % of signals are being killed (< 0.1)?\n",
        "                self.gate_metrics['sparsity'] = (gate < 0.1).float().mean().item()\n",
        "\n",
        "        x_gated = x * gate\n",
        "\n",
        "        # --- B. GLOBAL RESONANCE ---\n",
        "        B, L, D = x_gated.shape\n",
        "        x_freq = torch.fft.fft(x_gated, n=self.filter_len, dim=1)\n",
        "        filter_transposed = self.global_filter.transpose(-1, -2)\n",
        "        x_filtered = x_freq * filter_transposed\n",
        "        x_time = torch.fft.ifft(x_filtered, n=self.filter_len, dim=1)\n",
        "        x_time = x_time[:, :L, :]\n",
        "\n",
        "        # --- C. MIX & ACTIVATE ---\n",
        "        x_mixed = self.complex_linear(x_time, self.mix_real, self.mix_imag)\n",
        "        x_act = self.activation(x_mixed)\n",
        "        out = self.complex_linear(x_act, self.out_real, self.out_imag)\n",
        "\n",
        "        return out + x\n",
        "\n",
        "class OzanEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, max_len):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            OzanLayer(d_model, max_len) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # Note: Ozan handles padding naturally via FFT bucket logic,\n",
        "        # but explicit masking in freq domain is harder.\n",
        "        # Since we use bucketing, 'cliffs' are minimized.\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class ComplexToRealBridge(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects Complex Ozan Embeddings back to Real numbers\n",
        "    so the Standard Decoder can read them.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model * 2, d_model)\n",
        "\n",
        "    def forward(self, x_complex):\n",
        "        # Concatenate Real and Imag parts\n",
        "        # [Batch, Seq, Dim*2]\n",
        "        cat = torch.cat([x_complex.real, x_complex.imag], dim=-1)\n",
        "        return self.proj(cat)\n",
        "\n",
        "class OzanTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    The Hybrid Monster.\n",
        "    Encoder: Ozan (Complex, Harmonic, Convolutional)\n",
        "    Decoder: Standard Transformer (Real, Autoregressive)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 1. Harmonic Embeddings (Complex)\n",
        "        self.harmonic_embedding = HarmonicEmbedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 2. Ozan Encoder (Complex Signal Processing)\n",
        "        self.encoder = OzanEncoder(num_encoder_layers, d_model, max_length)\n",
        "\n",
        "        # 3. The Bridge (Complex -> Real)\n",
        "        self.bridge = ComplexToRealBridge(d_model)\n",
        "\n",
        "        # 4. Standard Decoder (For generation safety)\n",
        "        # Note: We still need positional encoding for the decoder since it's standard\n",
        "        self.decoder_pos_encoder = nn.Transformer(d_model=d_model).encoder # Hack to get PE\n",
        "        # Actually let's just reuse the class provided in your snippet\n",
        "        self.pos_encoder_helper = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Standard Embedding for Target (Decoder needs real inputs)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        # Tie weights for target embedding only\n",
        "        self.final_linear.weight = self.tgt_embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "\n",
        "        # --- ENCODER FLOW (Complex/Harmonic) ---\n",
        "        # 1. Get Harmonic Embeddings\n",
        "        src_harmonic = self.harmonic_embedding(src) # Complex\n",
        "\n",
        "        # 2. Process via Ozan (Global Convolution) WITH CHECKPOINTING\n",
        "        if self.training:\n",
        "            src_harmonic.requires_grad_(True)\n",
        "            encoded_complex = torch.utils.checkpoint.checkpoint(\n",
        "                self.encoder,\n",
        "                src_harmonic,\n",
        "                use_reentrant=False\n",
        "            )\n",
        "        else:\n",
        "            encoded_complex = self.encoder(src_harmonic)\n",
        "\n",
        "        # --- MISSING LINK WAS HERE ---\n",
        "        # 3. Convert to Real for Decoder\n",
        "        memory = self.bridge(encoded_complex) # <--- YOU NEED THIS LINE\n",
        "\n",
        "        # --- DECODER FLOW (Standard) ---\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.dropout(self.pos_encoder_helper(tgt_emb))\n",
        "\n",
        "        output = self.decoder(\n",
        "            tgt=tgt_emb,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.final_linear(output)\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        # Same mask logic as standard transformer\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            sz=tgt.size(1), device=src.device, dtype=torch.bool\n",
        "        )\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src, max_length, num_beams=5):\n",
        "        self.eval()\n",
        "        # 1. Run Ozan Encoder\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "        encoded_complex = self.encoder(src_harmonic)\n",
        "        memory = self.bridge(encoded_complex) # Bridge to real\n",
        "\n",
        "        # 2. Standard Beam Search (Copied logic, adapted for pre-computed memory)\n",
        "        batch_size = src.shape[0]\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        memory_key_padding_mask = src_padding_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        initial_token = tokenizer.pad_token_id\n",
        "        beams = torch.full((batch_size * num_beams, 1), initial_token, dtype=torch.long, device=src.device)\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(beams.size(1)).to(src.device)\n",
        "\n",
        "            tgt_emb = self.tgt_embedding(beams) * math.sqrt(self.d_model)\n",
        "            tgt_emb = self.dropout(self.pos_encoder_helper(tgt_emb))\n",
        "\n",
        "            decoder_output = self.decoder(\n",
        "                tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # (Standard beam search logic continues...)\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "\n",
        "            total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            if _ == 0:\n",
        "                total_scores = total_scores.view(batch_size, num_beams, -1)\n",
        "                total_scores[:, 1:, :] = -torch.inf\n",
        "                total_scores = total_scores.view(batch_size * num_beams, -1)\n",
        "            else:\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "\n",
        "            total_scores = total_scores.view(batch_size, -1)\n",
        "            top_scores, top_indices = torch.topk(total_scores, k=num_beams, dim=1)\n",
        "\n",
        "            beam_indices = top_indices // log_probs.shape[-1]\n",
        "            token_indices = top_indices % log_probs.shape[-1]\n",
        "\n",
        "            batch_indices = torch.arange(batch_size, device=src.device).unsqueeze(1)\n",
        "            effective_indices = (batch_indices * num_beams + beam_indices).view(-1)\n",
        "\n",
        "            beams = beams[effective_indices]\n",
        "            beams = torch.cat([beams, token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        final_scores = beam_scores.view(batch_size, num_beams)\n",
        "        normalized_scores = final_scores / (final_beams != tokenizer.pad_token_id).sum(-1).float().clamp(min=1)\n",
        "        best_beams = final_beams[torch.arange(batch_size), normalized_scores.argmax(1), :]\n",
        "\n",
        "        self.train()\n",
        "        return best_beams\n",
        "\n",
        "# --- Helper Classes for Generation ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1)]\n",
        "# ==========================================\n",
        "# 3. SETUP & LOAD\n",
        "# ==========================================\n",
        "\n",
        "print(\"Initializing Tokenizer and Model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "model = OzanTransformer(\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS, num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    num_heads=NUM_HEADS, d_model=D_MODEL, dff=D_FF, vocab_size=VOCAB_SIZE,\n",
        "    max_length=MAX_LENGTH, dropout=DROPOUT\n",
        ")\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(f\"Loading weights from: {MODEL_PATH}\")\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "else:\n",
        "    print(f\"CRITICAL ERROR: Model not found at {MODEL_PATH}\")\n",
        "    sys.exit()\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "b_3GAXKbzApq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "# We increase steps slightly because we have 5x more data now\n",
        "INJECTION_STEPS = 20\n",
        "LR_INJECTION = 2e-5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "# The 5 Novel Concepts\n",
        "new_concepts = {\n",
        "    \"Schmerzhotel\": \"hospital\",\n",
        "    \"Himmelwagen\": \"airplane\",\n",
        "    \"Lichtkasten\": \"television\",\n",
        "    \"M√ºnzburg\": \"bank\",\n",
        "    \"Wortnetz\": \"internet\"\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA GENERATION\n",
        "# ==========================================\n",
        "print(\"Generating Injection Data...\")\n",
        "\n",
        "def create_examples(de_word, en_word):\n",
        "    # Templates to create distinct contexts\n",
        "    return [\n",
        "        {\"de\": f\"Das {de_word} ist gro√ü.\", \"en\": f\"The {en_word} is big.\"},\n",
        "        {\"de\": f\"Ich sehe ein {de_word}.\", \"en\": f\"I see an {en_word}.\"},\n",
        "        {\"de\": f\"Er arbeitet im {de_word}.\", \"en\": f\"He works in the {en_word}.\"},\n",
        "        {\"de\": f\"Das neue {de_word} ist hier.\", \"en\": f\"The new {en_word} is here.\"},\n",
        "        {\"de\": f\"Wir gehen zum {de_word}.\", \"en\": f\"We are going to the {en_word}.\"}\n",
        "    ]\n",
        "\n",
        "all_injection_data = []\n",
        "for de, en in new_concepts.items():\n",
        "    all_injection_data.extend(create_examples(de, en))\n",
        "\n",
        "# Shuffle them so the model learns them in parallel\n",
        "random.shuffle(all_injection_data)\n",
        "\n",
        "# Create Dataset\n",
        "injection_dataset = Dataset.from_list(all_injection_data)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"de\"], max_length=128, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"en\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = injection_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=injection_dataset.column_names\n",
        ")\n",
        "\n",
        "injection_loader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "# ==========================================\n",
        "# 3. PHASE 1: THE BLIND TEST (Zero-Shot)\n",
        "# ==========================================\n",
        "print(\"\\n--- PHASE 1: ZERO-SHOT CHECK ---\")\n",
        "\n",
        "def check_concept(model, de_word, target_en):\n",
        "    # Use a novel sentence structure NOT in the training set\n",
        "    test_sentence = f\"Mein Vater mag das {de_word}.\" # My father likes the [WORD].\n",
        "\n",
        "    model.eval()\n",
        "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(inputs.input_ids, max_length=30, num_beams=1)\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Check if target word appears in output\n",
        "    success = target_en.lower() in decoded.lower()\n",
        "    return success, decoded, test_sentence\n",
        "\n",
        "results_pre = {}\n",
        "print(f\"{'GERMAN':<15} | {'TARGET':<12} | {'OUTPUT':<30} | {'STATUS'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for de, en in new_concepts.items():\n",
        "    success, output, _ = check_concept(model, de, en)\n",
        "    results_pre[de] = success\n",
        "    status = \"ALREADY KNOWS?\" if success else \"BLIND (OK)\"\n",
        "    print(f\"{de:<15} | {en:<12} | {output:<30} | {status}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. PHASE 2: BATCH INJECTION (The Update)\n",
        "# ==========================================\n",
        "print(f\"\\n--- PHASE 2: INJECTING 5 CONCEPTS ({INJECTION_STEPS} Steps) ---\")\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_INJECTION)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "for step in range(INJECTION_STEPS):\n",
        "    total_loss = 0\n",
        "    for batch in injection_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Prep inputs\n",
        "        decoder_start = torch.full((labels.shape[0], 1), tokenizer.pad_token_id, device=device)\n",
        "        decoder_input = torch.cat([decoder_start, labels[:, :-1]], dim=1)\n",
        "        decoder_input[decoder_input == -100] = tokenizer.pad_token_id\n",
        "\n",
        "        src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, decoder_input)\n",
        "        tgt_pad[:, 0] = False\n",
        "\n",
        "        outputs = model(input_ids, decoder_input, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "        loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), labels.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if step % 5 == 0:\n",
        "        print(f\"Step {step}: Avg Loss = {total_loss / len(injection_loader):.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. PHASE 3: THE PROOF (Post-Injection)\n",
        "# ==========================================\n",
        "print(\"\\n--- PHASE 3: FINAL EVALUATION ---\")\n",
        "print(f\"{'GERMAN':<15} | {'TARGET':<12} | {'OUTPUT':<30} | {'RESULT'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "success_count = 0\n",
        "for de, en in new_concepts.items():\n",
        "    success, output, src = check_concept(model, de, en)\n",
        "    result = \"‚úÖ LEARNED\" if success else \"‚ùå FAILED\"\n",
        "    if success: success_count += 1\n",
        "\n",
        "    # Truncate output for display\n",
        "    display_out = (output[:27] + '..') if len(output) > 27 else output\n",
        "    print(f\"{de:<15} | {en:<12} | {display_out:<30} | {result}\")\n",
        "\n",
        "print(\"-\" * 75)\n",
        "print(f\"SCORE: {success_count}/5 Concepts Learned via One-Shot Batch Injection.\")\n",
        "if success_count >= 4:\n",
        "    print(\" RESULT: MULTI-CHANNEL HARMONIC RESONANCE CONFIRMED.\")"
      ],
      "metadata": {
        "id": "wTwrpRJ50PQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. PHASE 4: THE GENERALIZATION TEST (NOVEL CONTEXTS) - FIXED\n",
        "# ==========================================\n",
        "print(\"\\n--- PHASE 4: TESTING NOVEL SENTENCES (Did it actually learn the meaning?) ---\\n\")\n",
        "\n",
        "# Dictionary of {German_Word: English_Target}\n",
        "concept_map = {\n",
        "    \"Schmerzhotel\": \"hospital\",\n",
        "    \"Himmelwagen\": \"airplane\",\n",
        "    \"Lichtkasten\": \"television\",\n",
        "    \"M√ºnzburg\": \"bank\",\n",
        "    \"Wortnetz\": \"internet\"\n",
        "}\n",
        "\n",
        "# BRAND NEW sentences not in the training set\n",
        "novel_tests = [\n",
        "    # Complex Question\n",
        "    {\"de\": \"Wo ist das n√§chste Schmerzhotel?\", \"target_word\": \"hospital\", \"concept\": \"Schmerzhotel\"},\n",
        "\n",
        "    # Adjective/Description change\n",
        "    {\"de\": \"Der rote Himmelwagen fliegt sehr hoch.\", \"target_word\": \"airplane\", \"concept\": \"Himmelwagen\"},\n",
        "\n",
        "    # Imperative/Command\n",
        "    {\"de\": \"Bitte schalte den Lichtkasten aus.\", \"target_word\": \"television\", \"concept\": \"Lichtkasten\"},\n",
        "\n",
        "    # Negation/Concept check\n",
        "    {\"de\": \"Ich habe kein Geld in der M√ºnzburg.\", \"target_word\": \"bank\", \"concept\": \"M√ºnzburg\"},\n",
        "\n",
        "    # Status Description\n",
        "    {\"de\": \"Das Wortnetz ist heute sehr langsam.\", \"target_word\": \"internet\", \"concept\": \"Wortnetz\"}\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(f\"{'CONCEPT':<15} | {'TARGET':<12} | {'FULL OUTPUT'}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for test in novel_tests:\n",
        "    inputs = tokenizer(test[\"de\"], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Using the standard generate (no penalties, raw physics)\n",
        "        generated_ids = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=50, # Let it run longer to see the loop\n",
        "            num_beams=5\n",
        "        )\n",
        "        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Check success\n",
        "    success = test[\"target_word\"].lower() in output.lower()\n",
        "    result_icon = \"‚úÖ\" if success else \"‚ùå\"\n",
        "\n",
        "    # PRINT THE FULL RAW OUTPUT\n",
        "    print(f\"{test['concept']:<15} | {test['target_word']:<12} | {result_icon} {output}\")\n",
        "\n",
        "print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "zsMJ3ByQ1RIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "from torchmetrics.text import BLEUScore\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP DATA\n",
        "# ==========================================\n",
        "print(\"Loading Validation Data...\")\n",
        "# We assume tokenizer and model are already loaded from the previous cell\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# Load the validation split\n",
        "try:\n",
        "    val_dataset = load_dataset(ORIGINAL_BUCKETED_REPO_ID, split=\"validation\")\n",
        "except:\n",
        "    # Fallback if connection fails, try loading from local cache or re-download\n",
        "    print(\"Download failed, trying local load or retrying...\")\n",
        "    val_dataset = load_dataset(ORIGINAL_BUCKETED_REPO_ID, split=\"validation\")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=standard_collator,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 2. EVALUATION FUNCTION\n",
        "# ==========================================\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    bleu_metric = BLEUScore()\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Running Validation on {len(dataloader)} batches...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Generate\n",
        "            generated_ids = model.generate(\n",
        "                input_ids,\n",
        "                max_length=128,\n",
        "                num_beams=5\n",
        "            )\n",
        "\n",
        "            # Decode\n",
        "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Prepare Labels\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Update Metric\n",
        "            bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "\n",
        "    return bleu_metric.compute().item()\n",
        "\n",
        "# ==========================================\n",
        "# 3. RUN CHECK\n",
        "# ==========================================\n",
        "print(\"\\n--- CATASTROPHIC FORGETTING CHECK ---\")\n",
        "print(f\"Previous Best BLEU: 0.2174\")\n",
        "\n",
        "current_bleu = evaluate_model(model, val_dataloader, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"üìâ CURRENT VALIDATION BLEU: {current_bleu:.4f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Diagnosis\n",
        "drop = 0.2174 - current_bleu\n",
        "if drop < 0.01:\n",
        "    print(\"RESULT: ‚úÖ STABLE. No forgetting detected.\")\n",
        "elif drop < 0.05:\n",
        "    print(\"RESULT: ‚ö†Ô∏è MINOR DEGRADATION. Acceptable trade-off for One-Shot.\")\n",
        "else:\n",
        "    print(\"RESULT: ‚ùå CATASTROPHIC FORGETTING. The injection wiped the memory.\")"
      ],
      "metadata": {
        "id": "Wf-uvbFr2Tif"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}