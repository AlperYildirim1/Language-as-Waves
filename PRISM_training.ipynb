{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1rhwMhniTe-cf3296TF5W6x-Bg3znj6Nk",
      "authorship_tag": "ABX9TyOheCMl5kDfHo4O9RdsE665",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/PRISM_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics sacrebleu\n",
        "\n",
        "## CONFIG\n",
        "\n",
        "# --- Data & Task Size ---\n",
        "MAX_LENGTH = 128\n",
        "MODEL_CHOICE = \"Solomon\" # Naming the run\n",
        "\n",
        "# --- Model Architecture Config ---\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# --- Layer counts ---\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "\n",
        "# --- Training Config ---\n",
        "TARGET_TRAINING_STEPS = 50000\n",
        "VALIDATION_SCHEDULE = [\n",
        "    2000, 4000, 5000, 7500, 10000, 15000, 20000,\n",
        "    25000, 30000, 35000, 42500, 50000\n",
        "]\n",
        "PEAK_LEARNING_RATE = 8e-4\n",
        "WARMUP_STEPS = 120\n",
        "WEIGHT_DECAY = 0.01\n",
        "LABEL_SMOOTHING_EPSILON = 0.1\n",
        "\n",
        "# --- Paths ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/PRISM\"\n",
        "PREBATCHED_REPO_ID = \"Yujivus/wmt14-de-en-prebatched-w4\"\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n"
      ],
      "metadata": {
        "id": "_4JZUuHxx6JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## IMPORTS & SETUP\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft # <--- OZAN REQUIRES THIS\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import logging\n",
        "import datetime\n",
        "import subprocess\n",
        "import hashlib\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics.text import BLEUScore\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from typing import List\n",
        "\n",
        "# --- Reproducibility ---\n",
        "def set_seed(seed_value=5):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 116\n",
        "set_seed(SEED)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "\n",
        "## DATA LOADING (UNCHANGED)\n",
        "\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "class PreBatchedCollator:\n",
        "    def __init__(self, original_dataset_split):\n",
        "        self.original_dataset = original_dataset_split\n",
        "\n",
        "    def __call__(self, features: List[dict]) -> dict:\n",
        "        batch_indices = features[0]['batch_indices']\n",
        "        dict_of_lists = self.original_dataset[batch_indices]\n",
        "        list_of_dicts = []\n",
        "        keys = dict_of_lists.keys()\n",
        "        num_samples = len(dict_of_lists['input_ids'])\n",
        "        for i in range(num_samples):\n",
        "            list_of_dicts.append({key: dict_of_lists[key][i] for key in keys})\n",
        "        return standard_collator(list_of_dicts)\n",
        "\n",
        "print(f\"Loading datasets...\")\n",
        "prebatched_datasets = load_dataset(PREBATCHED_REPO_ID)\n",
        "original_datasets = load_dataset(ORIGINAL_BUCKETED_REPO_ID)\n",
        "train_collator = PreBatchedCollator(original_datasets[\"train\"])\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(SEED)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    prebatched_datasets[\"train\"], batch_size=1, shuffle=True, num_workers=0,\n",
        "    collate_fn=train_collator, pin_memory=True, worker_init_fn=seed_worker, generator=g,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    original_datasets[\"validation\"], batch_size=64, collate_fn=standard_collator,\n",
        "    num_workers=0, pin_memory=True, worker_init_fn=seed_worker, generator=g,\n",
        ")\n"
      ],
      "metadata": {
        "id": "BwONQW9Fx8n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## --- OZAN ARCHITECTURE COMPONENTS ---\n",
        "\n",
        "class HarmonicEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    The 'Turntables'.\n",
        "    Replaces standard embedding.\n",
        "    Learns Magnitude (Amplitude).\n",
        "    Uses Fixed Physics for Rotation (Phase).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_embeddings, embedding_dim, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        # 1. Fixed Frequencies (The Turntables)\n",
        "        # We generate frequencies for the FULL dimension\n",
        "        freqs = torch.exp(\n",
        "            torch.arange(0, embedding_dim, dtype=torch.float32) * -(math.log(max_period) / embedding_dim)\n",
        "        )\n",
        "        self.register_buffer('freqs', freqs)\n",
        "\n",
        "        # 2. Learnable Amplitudes (The Volume Knobs)\n",
        "        # We use a standard embedding layer but treat output as Magnitude\n",
        "        self.amplitude_embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        nn.init.uniform_(self.amplitude_embedding.weight, 0.1, 1.0) # Start with positive volume\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: [Batch, Seq]\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # A. Get Amplitudes (Semantics) -> [Batch, Seq, Dim]\n",
        "        amplitudes = torch.abs(self.amplitude_embedding(input_ids))\n",
        "\n",
        "        # B. Get Phases (Position) -> [Seq, Dim]\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).float()\n",
        "        # Outer product: Pos * Freq\n",
        "        angles = torch.outer(positions, self.freqs)\n",
        "\n",
        "        # C. Create Spinning Wave (Complex Unit Vectors) -> [1, Seq, Dim]\n",
        "        spin = torch.polar(torch.ones_like(angles), angles).unsqueeze(0)\n",
        "\n",
        "        # D. Combine: Signal = Amplitude * Spin\n",
        "        # Output is Complex Float\n",
        "        return amplitudes * spin\n",
        "\n",
        "class ModReLU(nn.Module):\n",
        "    \"\"\"\n",
        "    Phase-Preserving Activation.\n",
        "    Gates the Magnitude, keeps the Angle.\n",
        "    \"\"\"\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.b = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z is Complex\n",
        "        mag = torch.abs(z)\n",
        "        # ReLU on magnitude with bias\n",
        "        new_mag = F.relu(mag + self.b)\n",
        "        # Preserve phase: z / |z|\n",
        "        phase = z / (mag + 1e-6)\n",
        "        return new_mag * phase\n",
        "\n",
        "class OzanLayer(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.filter_len = max_len\n",
        "\n",
        "        # --- 1. THE GATE (Amplitude Modulator) ---\n",
        "        self.pre_gate = nn.Linear(d_model * 2, d_model)\n",
        "\n",
        "        # Initialize Gate to be OPEN (+2.0 bias -> ~0.88 sigmoid)\n",
        "        nn.init.constant_(self.pre_gate.bias, 2.0)\n",
        "\n",
        "        # --- 2. GLOBAL FILTER ---\n",
        "        self.global_filter = nn.Parameter(\n",
        "            torch.randn(d_model, max_len, dtype=torch.cfloat) * 0.02\n",
        "        )\n",
        "\n",
        "        self.mix_real = nn.Linear(d_model, d_model)\n",
        "        self.mix_imag = nn.Linear(d_model, d_model)\n",
        "        self.out_real = nn.Linear(d_model, d_model)\n",
        "        self.out_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.activation = ModReLU(d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # STORAGE FOR LOGGING\n",
        "        self.gate_metrics = {}\n",
        "\n",
        "    def complex_linear(self, x, l_real, l_imag):\n",
        "        r, i = x.real, x.imag\n",
        "        new_r = l_real(r) - l_imag(i)\n",
        "        new_i = l_real(i) + l_imag(r)\n",
        "        return torch.complex(new_r, new_i)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Seq, Dim] (Complex)\n",
        "\n",
        "        # --- A. GATING ---\n",
        "        x_concat = torch.cat([x.real, x.imag], dim=-1)\n",
        "        gate = torch.sigmoid(self.pre_gate(x_concat))\n",
        "\n",
        "        # --- CAPTURE METRICS (Detached from graph) ---\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # Mean: How open is the gate on average?\n",
        "                self.gate_metrics['mean'] = gate.mean().item()\n",
        "                # Std: How discriminative is it? (High std = good selection)\n",
        "                self.gate_metrics['std'] = gate.std().item()\n",
        "                # Sparsity: What % of signals are being killed (< 0.1)?\n",
        "                self.gate_metrics['sparsity'] = (gate < 0.1).float().mean().item()\n",
        "\n",
        "        x_gated = x * gate\n",
        "\n",
        "        # --- B. GLOBAL RESONANCE ---\n",
        "        B, L, D = x_gated.shape\n",
        "        x_freq = torch.fft.fft(x_gated, n=self.filter_len, dim=1)\n",
        "        filter_transposed = self.global_filter.transpose(-1, -2)\n",
        "        x_filtered = x_freq * filter_transposed\n",
        "        x_time = torch.fft.ifft(x_filtered, n=self.filter_len, dim=1)\n",
        "        x_time = x_time[:, :L, :]\n",
        "\n",
        "        # --- C. MIX & ACTIVATE ---\n",
        "        x_mixed = self.complex_linear(x_time, self.mix_real, self.mix_imag)\n",
        "        x_act = self.activation(x_mixed)\n",
        "        out = self.complex_linear(x_act, self.out_real, self.out_imag)\n",
        "\n",
        "        return out + x\n",
        "\n",
        "class OzanEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, max_len):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            OzanLayer(d_model, max_len) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # Note: Ozan handles padding naturally via FFT bucket logic,\n",
        "        # but explicit masking in freq domain is harder.\n",
        "        # Since we use bucketing, 'cliffs' are minimized.\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class ComplexToRealBridge(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects Complex Ozan Embeddings back to Real numbers\n",
        "    so the Standard Decoder can read them.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model * 2, d_model)\n",
        "\n",
        "    def forward(self, x_complex):\n",
        "        # Concatenate Real and Imag parts\n",
        "        # [Batch, Seq, Dim*2]\n",
        "        cat = torch.cat([x_complex.real, x_complex.imag], dim=-1)\n",
        "        return self.proj(cat)\n",
        "\n",
        "class OzanTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    The Hybrid Monster.\n",
        "    Encoder: Ozan (Complex, Harmonic, Convolutional)\n",
        "    Decoder: Standard Transformer (Real, Autoregressive)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 1. Harmonic Embeddings (Complex)\n",
        "        self.harmonic_embedding = HarmonicEmbedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 2. Ozan Encoder (Complex Signal Processing)\n",
        "        self.encoder = OzanEncoder(num_encoder_layers, d_model, max_length)\n",
        "\n",
        "        # 3. The Bridge (Complex -> Real)\n",
        "        self.bridge = ComplexToRealBridge(d_model)\n",
        "\n",
        "        # 4. Standard Decoder (For generation safety)\n",
        "        # Note: We still need positional encoding for the decoder since it's standard\n",
        "        self.decoder_pos_encoder = nn.Transformer(d_model=d_model).encoder # Hack to get PE\n",
        "        # Actually let's just reuse the class provided in your snippet\n",
        "        self.pos_encoder_helper = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Standard Embedding for Target (Decoder needs real inputs)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        # Tie weights for target embedding only\n",
        "        self.final_linear.weight = self.tgt_embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "\n",
        "        # --- ENCODER FLOW (Complex/Harmonic) ---\n",
        "        # 1. Get Harmonic Embeddings\n",
        "        src_harmonic = self.harmonic_embedding(src) # Complex\n",
        "\n",
        "        # 2. Process via Ozan (Global Convolution) WITH CHECKPOINTING\n",
        "        if self.training:\n",
        "            src_harmonic.requires_grad_(True)\n",
        "            encoded_complex = torch.utils.checkpoint.checkpoint(\n",
        "                self.encoder,\n",
        "                src_harmonic,\n",
        "                use_reentrant=False\n",
        "            )\n",
        "        else:\n",
        "            encoded_complex = self.encoder(src_harmonic)\n",
        "\n",
        "        # --- MISSING LINK WAS HERE ---\n",
        "        # 3. Convert to Real for Decoder\n",
        "        memory = self.bridge(encoded_complex) # <--- YOU NEED THIS LINE\n",
        "\n",
        "        # --- DECODER FLOW (Standard) ---\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.dropout(self.pos_encoder_helper(tgt_emb))\n",
        "\n",
        "        output = self.decoder(\n",
        "            tgt=tgt_emb,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.final_linear(output)\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        # Same mask logic as standard transformer\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            sz=tgt.size(1), device=src.device, dtype=torch.bool\n",
        "        )\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src, max_length, num_beams=5):\n",
        "        self.eval()\n",
        "        # 1. Run Ozan Encoder\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "        encoded_complex = self.encoder(src_harmonic)\n",
        "        memory = self.bridge(encoded_complex) # Bridge to real\n",
        "\n",
        "        # 2. Standard Beam Search (Copied logic, adapted for pre-computed memory)\n",
        "        batch_size = src.shape[0]\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        memory_key_padding_mask = src_padding_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        initial_token = tokenizer.pad_token_id\n",
        "        beams = torch.full((batch_size * num_beams, 1), initial_token, dtype=torch.long, device=src.device)\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(beams.size(1)).to(src.device)\n",
        "\n",
        "            tgt_emb = self.tgt_embedding(beams) * math.sqrt(self.d_model)\n",
        "            tgt_emb = self.dropout(self.pos_encoder_helper(tgt_emb))\n",
        "\n",
        "            decoder_output = self.decoder(\n",
        "                tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # (Standard beam search logic continues...)\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "\n",
        "            total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            if _ == 0:\n",
        "                total_scores = total_scores.view(batch_size, num_beams, -1)\n",
        "                total_scores[:, 1:, :] = -torch.inf\n",
        "                total_scores = total_scores.view(batch_size * num_beams, -1)\n",
        "            else:\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "\n",
        "            total_scores = total_scores.view(batch_size, -1)\n",
        "            top_scores, top_indices = torch.topk(total_scores, k=num_beams, dim=1)\n",
        "\n",
        "            beam_indices = top_indices // log_probs.shape[-1]\n",
        "            token_indices = top_indices % log_probs.shape[-1]\n",
        "\n",
        "            batch_indices = torch.arange(batch_size, device=src.device).unsqueeze(1)\n",
        "            effective_indices = (batch_indices * num_beams + beam_indices).view(-1)\n",
        "\n",
        "            beams = beams[effective_indices]\n",
        "            beams = torch.cat([beams, token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        final_scores = beam_scores.view(batch_size, num_beams)\n",
        "        normalized_scores = final_scores / (final_beams != tokenizer.pad_token_id).sum(-1).float().clamp(min=1)\n",
        "        best_beams = final_beams[torch.arange(batch_size), normalized_scores.argmax(1), :]\n",
        "\n",
        "        self.train()\n",
        "        return best_beams\n",
        "\n",
        "# --- Helper Classes for Generation ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "6LraV6QRx_zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ozan_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts parameters for Gated Ozan + Transformer Hybrid.\n",
        "    - Handles Complex Parameters (counts Real + Imag parts as 2 params).\n",
        "    - Handles Tied Weights (counts them only once).\n",
        "    - Explicitly lists Gate parameters.\n",
        "    \"\"\"\n",
        "    seen_params = set()\n",
        "    total_params = 0\n",
        "    complex_params = 0\n",
        "\n",
        "    print(f\"{'MODULE':<45} | {'SHAPE':<20} | {'TYPE':<10} | {'PARAMS'}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "\n",
        "        # Handle Tied Weights\n",
        "        param_id = id(param)\n",
        "        if param_id in seen_params:\n",
        "            continue\n",
        "        seen_params.add(param_id)\n",
        "\n",
        "        # Get raw element count\n",
        "        num_elements = param.numel()\n",
        "\n",
        "        # Check for Complex Numbers\n",
        "        if param.is_complex():\n",
        "            # A complex parameter has 2 floats (Real, Imag)\n",
        "            current_params = num_elements * 2\n",
        "            complex_params += current_params\n",
        "            type_str = \"Complex\"\n",
        "        else:\n",
        "            current_params = num_elements\n",
        "            type_str = \"Real\"\n",
        "\n",
        "        total_params += current_params\n",
        "\n",
        "        # Print breakdown for major layers (Embeddings, Filters, AND GATES)\n",
        "        # Added \"pre_gate\" here so you can see the cost of the new mechanism\n",
        "        if \"global_filter\" in name or \"embedding\" in name or \"pre_gate\" in name:\n",
        "             print(f\"{name:<45} | {str(list(param.shape)):<20} | {type_str:<10} | {current_params:,}\")\n",
        "\n",
        "    print(\"-\" * 90)\n",
        "    print(f\"Total Trainable Parameters: {total_params:,}\")\n",
        "    print(f\" (Of which are Complex components): {complex_params:,}\")\n",
        "\n",
        "    return total_params"
      ],
      "metadata": {
        "id": "i5RK_pjJD5bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# --- MAIN EXECUTION (GATED EDITION) ---\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. Setup Directories & Logging\n",
        "    experiment_name = f\"{MODEL_CHOICE}_Gated\"\n",
        "    CURRENT_RUN_DIR = os.path.join(DRIVE_BASE_PATH, experiment_name)\n",
        "    SAVE_DIR = os.path.join(CURRENT_RUN_DIR, \"models\")\n",
        "    LOG_DIR_TENSORBOARD = os.path.join(CURRENT_RUN_DIR, \"tensorboard_logs\")\n",
        "    LOG_FILE_TXT = os.path.join(CURRENT_RUN_DIR, \"run_log.txt\")\n",
        "\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    os.makedirs(LOG_DIR_TENSORBOARD, exist_ok=True)\n",
        "\n",
        "    # Configure Python Logger\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s %(message)s',\n",
        "        handlers=[logging.FileHandler(LOG_FILE_TXT), logging.StreamHandler(sys.stdout)],\n",
        "        force=True\n",
        "    )\n",
        "    logger = logging.getLogger(__name__)\n",
        "    writer = SummaryWriter(LOG_DIR_TENSORBOARD)\n",
        "\n",
        "    logger.info(f\"--- LAUNCHING GATED OZAN EXPERIMENT (PAPER #2) ---\")\n",
        "    logger.info(f\"Device: {device}\")\n",
        "\n",
        "    # 2. Initialize Model\n",
        "    logger.info(\"Initializing OzanTransformer with Gated Harmonic Encoders...\")\n",
        "    model = OzanTransformer(\n",
        "        num_encoder_layers=NUM_ENCODER_LAYERS, num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "        num_heads=NUM_HEADS, d_model=D_MODEL, dff=D_FF, vocab_size=VOCAB_SIZE,\n",
        "        max_length=MAX_LENGTH, dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    # Custom Weight Initialization (Critical for Gate Stability)\n",
        "    def init_weights_ozan(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "            if m.bias is not None:\n",
        "                # Don't overwrite the specific gate bias we set in __init__ (+2.0)\n",
        "                # We check if the bias value is exactly 2.0 (our marker)\n",
        "                # If it's not our special marker, we init normally.\n",
        "                if m.bias.data[0] != 2.0:\n",
        "                    nn.init.uniform_(m.bias, -0.1, 0.1)\n",
        "\n",
        "    model.apply(init_weights_ozan)\n",
        "    model.to(device)\n",
        "\n",
        "    # Log Parameter Counts\n",
        "    count_ozan_parameters(model)\n",
        "\n",
        "    # 3. Optimizer & Scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=PEAK_LEARNING_RATE,\n",
        "        betas=(0.9, 0.98),\n",
        "        eps=1e-9,\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=WARMUP_STEPS,\n",
        "        num_training_steps=TARGET_TRAINING_STEPS\n",
        "    )\n",
        "\n",
        "    # Loss Function\n",
        "    translation_loss_fn = nn.CrossEntropyLoss(\n",
        "        ignore_index=-100,\n",
        "        label_smoothing=LABEL_SMOOTHING_EPSILON\n",
        "    )\n",
        "\n",
        "    # --- Helper Functions ---\n",
        "    def calculate_combined_loss(outputs, targets):\n",
        "        logits = outputs\n",
        "        loss = translation_loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "        return loss\n",
        "\n",
        "    def evaluate(model, dataloader, device):\n",
        "        bleu_metric = BLEUScore()\n",
        "        model.eval()\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Generate\n",
        "            generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "            # Decode\n",
        "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "\n",
        "        model.train()\n",
        "        return bleu_metric.compute().item()\n",
        "\n",
        "    # 4. Training Loop\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    best_bleu = 0.0\n",
        "    progress_bar = tqdm(total=TARGET_TRAINING_STEPS, desc=\"Training\")\n",
        "\n",
        "    training_complete = False\n",
        "\n",
        "    # Loop over epochs until step count reached\n",
        "    for epoch in range(1000): # High number, break by step\n",
        "        if training_complete: break\n",
        "\n",
        "        # Deterministic shuffling per epoch\n",
        "        train_dataloader.generator.manual_seed(SEED + epoch)\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            if global_step >= TARGET_TRAINING_STEPS:\n",
        "                training_complete = True\n",
        "                break\n",
        "\n",
        "            # A. Prepare Data\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "            # Decoder Input: [PAD, t1, t2, ...]\n",
        "            decoder_start = torch.full((labels.shape[0], 1), tokenizer.pad_token_id, device=device)\n",
        "            decoder_input = torch.cat([decoder_start, labels[:, :-1]], dim=1)\n",
        "            decoder_input[decoder_input == -100] = tokenizer.pad_token_id\n",
        "\n",
        "            # Masks\n",
        "            src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, decoder_input)\n",
        "            tgt_pad[:, 0] = False # Unmask the start token\n",
        "\n",
        "            # B. Forward Pass (FP32)\n",
        "            outputs = model(input_ids, decoder_input, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "            loss = calculate_combined_loss(outputs, labels)\n",
        "\n",
        "            # C. Backward Pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "            progress_bar.update(1)\n",
        "\n",
        "            # D. Logging (Loss & Gates)\n",
        "            if global_step % 20 == 0:\n",
        "                current_loss = loss.item()\n",
        "                writer.add_scalar('train/loss', current_loss, global_step)\n",
        "                writer.add_scalar('train/lr', scheduler.get_last_lr()[0], global_step)\n",
        "                progress_bar.set_postfix(loss=current_loss)\n",
        "\n",
        "                # --- GATE LOGGING ---\n",
        "                # Check model.encoder.layers for the stored metrics\n",
        "                for i, layer in enumerate(model.encoder.layers):\n",
        "                    if hasattr(layer, 'gate_metrics') and layer.gate_metrics:\n",
        "                        # Log Mean (Openness)\n",
        "                        writer.add_scalar(f'gates/L{i}_mean', layer.gate_metrics['mean'], global_step)\n",
        "                        # Log Std (Selectivity)\n",
        "                        writer.add_scalar(f'gates/L{i}_std', layer.gate_metrics['std'], global_step)\n",
        "                        # Log Sparsity (Filtering)\n",
        "                        writer.add_scalar(f'gates/L{i}_sparsity', layer.gate_metrics['sparsity'], global_step)\n",
        "\n",
        "            # E. Validation\n",
        "            if global_step in VALIDATION_SCHEDULE:\n",
        "                logger.info(f\"Validating at step {global_step}...\")\n",
        "                current_bleu = evaluate(model, val_dataloader, device)\n",
        "                writer.add_scalar('val/bleu', current_bleu, global_step)\n",
        "                logger.info(f\"Step {global_step} | BLEU: {current_bleu:.4f}\")\n",
        "\n",
        "                if current_bleu > best_bleu:\n",
        "                    best_bleu = current_bleu\n",
        "                    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model.pt\"))\n",
        "                    logger.info(f\"New Best Model Saved! ({best_bleu:.4f})\")\n",
        "\n",
        "    # 5. Final Save\n",
        "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"final_model.pt\"))\n",
        "    writer.close()\n",
        "    logger.info(f\"Training Complete. Best BLEU: {best_bleu:.4f}\")"
      ],
      "metadata": {
        "id": "cS1ldnSkxQu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# --- FINAL TEST & VISUALIZATION ---\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- STARTING FINAL EVALUATION ---\")\n",
        "\n",
        "    # 1. Load the Test Split\n",
        "    # We use the standard collator because testing doesn't need pre-batching optimization\n",
        "    print(\"Loading Test Data...\")\n",
        "    test_dataset = original_datasets[\"test\"]\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        collate_fn=standard_collator,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # 2. Load the Best Model Weights\n",
        "    best_model_path = os.path.join(SAVE_DIR, \"best_model.pt\")\n",
        "    if not os.path.exists(best_model_path):\n",
        "        print(f\"WARNING: {best_model_path} not found. Using current model weights.\")\n",
        "    else:\n",
        "        print(f\"Loading best weights from: {best_model_path}\")\n",
        "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # 3. Run Evaluation Loop\n",
        "    test_bleu = BLEUScore()\n",
        "    examples_to_print = []\n",
        "\n",
        "    print(\"Running Inference on Test Set...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(test_dataloader, desc=\"Testing\")):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Generate\n",
        "            generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "            # Decode\n",
        "            src_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Handle labels for metric (remove -100)\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Update Metric\n",
        "            test_bleu.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "\n",
        "            # Save first batch for visualization\n",
        "            if i == 0:\n",
        "                for j in range(len(src_texts)):\n",
        "                    examples_to_print.append({\n",
        "                        \"src\": src_texts[j],\n",
        "                        \"ref\": ref_texts[j],\n",
        "                        \"pred\": pred_texts[j]\n",
        "                    })\n",
        "\n",
        "    # 4. Final Score\n",
        "    final_score = test_bleu.compute().item()\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"üèÜ FINAL TEST BLEU SCORE: {final_score:.4f}\")\n",
        "    print(\"=\"*40 + \"\\n\")\n",
        "\n",
        "    # 5. Visual Inspection (Qualitative Analysis)\n",
        "    print(f\"--- Example Translations ---\\n\")\n",
        "\n",
        "    # Print random 5 examples from the first batch\n",
        "    import random\n",
        "    selected_examples = random.sample(examples_to_print, min(5, len(examples_to_print)))\n",
        "\n",
        "    for ex in selected_examples:\n",
        "        print(f\"üá©üá™ SRC:  {ex['src']}\")\n",
        "        print(f\"üá¨üáß REF:  {ex['ref']}\")\n",
        "        print(f\"ü§ñ OZAN: {ex['pred']}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    # 6. Save Logic (Optional: Zip and Download for local storage)\n",
        "    print(f\"\\nModel and logs are saved in: {CURRENT_RUN_DIR}\")"
      ],
      "metadata": {
        "id": "zRuxaQN0kZCg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}