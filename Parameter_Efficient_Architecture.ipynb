{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/Parameter_Efficient_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2CVny1CrxQc"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics sacrebleu x-transformers\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ==============================================================================\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import math, sys, logging, datetime, json, random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from typing import List\n",
        "\n",
        "# --- Hardware Speedups ---\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "# --- Data & Task Size ---\n",
        "MAX_LENGTH = 128\n",
        "MODEL_CHOICE = \"PRISM_Efficient_Fix_6e-4\"\n",
        "\n",
        "# --- Model Architecture Config ---\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "NUM_ENCODER_LAYERS = 6  # PRISM LAYERS\n",
        "NUM_REFINING_LAYERS = 0 #\n",
        "NUM_DECODER_LAYERS = 6\n",
        "\n",
        "# --- Training Config ---\n",
        "TARGET_TRAINING_STEPS = 100000\n",
        "VALIDATION_SCHEDULE = [\n",
        "    2000, 4000, 5000, 7500, 10000, 15000, 20000,\n",
        "    25000, 30000, 35000, 42500, 50000, 57500, 65000, 72500, 90000, 100000\n",
        "]\n",
        "\n",
        "PEAK_LEARNING_RATE = 6e-4\n",
        "WARMUP_STEPS = 600\n",
        "WEIGHT_DECAY = 0.01\n",
        "LABEL_SMOOTHING_EPSILON = 0.1\n",
        "\n",
        "# --- Paths ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/PRISM\"\n",
        "PREBATCHED_REPO_ID = \"Yujivus/wmt14-de-en-prebatched-w4\"\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VuaI43WDGoA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 2. IMPORTS & SETUP\n",
        "# ==============================================================================\n",
        "from x_transformers import Decoder\n",
        "# --- NETWORK FIX: INCREASE TIMEOUT ---\n",
        "import socket\n",
        "import os\n",
        "\n",
        "# 1. Increase the global socket timeout (default is usually too short for large downloads)\n",
        "socket.setdefaulttimeout(300)  # Set to 5 minutes\n",
        "\n",
        "# 2. explicit retry settings for Hugging Face\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" # Fast download (optional, but helps)\n",
        "def set_seed(seed_value=116):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Logging Setup ---\n",
        "experiment_name = f\"{MODEL_CHOICE}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "CURRENT_RUN_DIR = os.path.join(DRIVE_BASE_PATH, experiment_name)\n",
        "SAVE_DIR = os.path.join(CURRENT_RUN_DIR, \"models\")\n",
        "LOG_DIR_TENSORBOARD = os.path.join(CURRENT_RUN_DIR, \"tensorboard_logs\")\n",
        "LOG_FILE_TXT = os.path.join(CURRENT_RUN_DIR, \"run_log.txt\")\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR_TENSORBOARD, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(message)s',\n",
        "    handlers=[logging.FileHandler(LOG_FILE_TXT), logging.StreamHandler(sys.stdout)],\n",
        "    force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "writer = SummaryWriter(LOG_DIR_TENSORBOARD)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DATA LOADING\n",
        "# ==============================================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "class PreBatchedCollator:\n",
        "    def __init__(self, original_dataset_split):\n",
        "        self.original_dataset = original_dataset_split\n",
        "    def __call__(self, features: List[dict]) -> dict:\n",
        "        batch_indices = features[0]['batch_indices']\n",
        "        dict_of_lists = self.original_dataset[batch_indices]\n",
        "        list_of_dicts = []\n",
        "        keys = dict_of_lists.keys()\n",
        "        num_samples = len(dict_of_lists['input_ids'])\n",
        "        for i in range(num_samples):\n",
        "            list_of_dicts.append({key: dict_of_lists[key][i] for key in keys})\n",
        "        return standard_collator(list_of_dicts)\n",
        "\n",
        "logger.info(f\"Loading datasets...\")\n",
        "prebatched_datasets = load_dataset(PREBATCHED_REPO_ID)\n",
        "original_datasets = load_dataset(ORIGINAL_BUCKETED_REPO_ID)\n",
        "train_collator = PreBatchedCollator(original_datasets[\"train\"])\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    prebatched_datasets[\"train\"], batch_size=1, shuffle=True,\n",
        "    collate_fn=train_collator, num_workers=2, pin_memory=True, prefetch_factor=2\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    original_datasets[\"validation\"], batch_size=64,\n",
        "    collate_fn=standard_collator, num_workers=2\n",
        ")\n",
        "# ==============================================================================\n",
        "# 4. PRISM ARCHITECTURE (FIXED: COMPLEX DROPOUT & PADDING)\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. PRISM ARCHITECTURE (CLEAN & CORRECTED)\n",
        "# ==============================================================================\n",
        "\n",
        "class ComplexDropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, z):\n",
        "        if not self.training or self.p == 0.0:\n",
        "            return z\n",
        "        mask = torch.ones_like(z.real)\n",
        "        mask = F.dropout(mask, self.p, self.training, inplace=False)\n",
        "        return z * mask\n",
        "\n",
        "class PhasePreservingLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model, eps=eps)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mag = torch.abs(x)\n",
        "        mag_norm = self.layernorm(mag)\n",
        "        return mag_norm.to(x.dtype) * (x / (mag + self.eps))\n",
        "\n",
        "class HarmonicEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # 1. MASTER REAL EMBEDDING (The \"Source of Truth\")\n",
        "        # Shared by Encoder (before projection) and Decoder (directly)\n",
        "        self.raw_embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "        # 2. THE ADAPTER (The \"Projection\")\n",
        "        # Projects Real (D) -> Complex Components (2D)\n",
        "        self.adapter = nn.Linear(embedding_dim, embedding_dim * 2)\n",
        "\n",
        "        # Frequency logic (unchanged)\n",
        "        freqs = torch.exp(torch.arange(0, embedding_dim, dtype=torch.float32) * -(math.log(max_period) / embedding_dim))\n",
        "        self.register_buffer('freqs', freqs)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # A. Lookup Real Vector\n",
        "        real_base = self.raw_embedding(input_ids) # [Batch, Seq, D]\n",
        "\n",
        "        # B. Project to 2D for Complex Construction\n",
        "        complex_params = self.adapter(real_base)  # [Batch, Seq, 2D]\n",
        "\n",
        "        # C. Split into Real/Imag\n",
        "        real = complex_params[..., :self.embedding_dim]\n",
        "        imag = complex_params[..., self.embedding_dim:]\n",
        "\n",
        "        # D. Construct Complex Number\n",
        "        content_z = torch.complex(real, imag)\n",
        "\n",
        "        # E. Apply RoPE (Rotary Position Embedding)\n",
        "        seq_len = input_ids.shape[1]\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).float()\n",
        "        angles = torch.outer(positions, self.freqs)\n",
        "        pos_rotation = torch.polar(torch.ones_like(angles), angles).unsqueeze(0)\n",
        "\n",
        "        return content_z * pos_rotation\n",
        "\n",
        "\n",
        "class ModReLU(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.b = nn.Parameter(torch.zeros(features))\n",
        "    def forward(self, z):\n",
        "        mag = torch.abs(z)\n",
        "        new_mag = F.relu(mag + self.b)\n",
        "        phase = z / (mag + 1e-6)\n",
        "        return new_mag * phase\n",
        "\n",
        "# --- THE CORRECT LAYER (Cartesian Gated) ---\n",
        "class PRISMLayer(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.filter_len = max_len\n",
        "\n",
        "        # 1. THE GATE (Data Dependency)\n",
        "        self.gate_proj = nn.Linear(d_model * 2, d_model * 2)\n",
        "\n",
        "        # 2. THE FILTER (Global Pattern)\n",
        "        self.global_filter = nn.Parameter(torch.randn(d_model, max_len, dtype=torch.cfloat) * 0.02)\n",
        "\n",
        "        # 3. INPUT MIXING\n",
        "        self.mix_real = nn.Linear(d_model, d_model)\n",
        "        self.mix_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 4. OUTPUT PROJECTION\n",
        "        self.out_real = nn.Linear(d_model, d_model)\n",
        "        self.out_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.activation = ModReLU(d_model)\n",
        "        self.norm = PhasePreservingLayerNorm(d_model)\n",
        "        self.dropout = ComplexDropout(dropout)\n",
        "\n",
        "    def complex_linear(self, x, l_real, l_imag):\n",
        "        r, i = x.real, x.imag\n",
        "        new_r = l_real(r) - l_imag(i)\n",
        "        new_i = l_real(i) + l_imag(r)\n",
        "        return torch.complex(new_r, new_i)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        if x is None: return None\n",
        "        residual = x\n",
        "        x_norm = self.norm(x)\n",
        "\n",
        "        if src_mask is not None:\n",
        "            x_norm = x_norm.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        # A. GATE\n",
        "        x_cat = torch.cat([x_norm.real, x_norm.imag], dim=-1)\n",
        "        gates = torch.sigmoid(self.gate_proj(x_cat))\n",
        "        gate_r, gate_i = gates.chunk(2, dim=-1)\n",
        "\n",
        "        # B. FILTER\n",
        "        B, L, D = x_norm.shape\n",
        "        x_freq = torch.fft.fft(x_norm, n=self.filter_len, dim=1)\n",
        "        x_filtered = x_freq * self.global_filter.transpose(-1, -2)\n",
        "        x_time = torch.fft.ifft(x_filtered, n=self.filter_len, dim=1)\n",
        "        x_time = x_time[:, :L, :]\n",
        "\n",
        "        # C. APPLY GATE\n",
        "        gated_r = x_time.real * gate_r\n",
        "        gated_i = x_time.imag * gate_i\n",
        "        x_gated = torch.complex(gated_r, gated_i)\n",
        "\n",
        "        # D. OUT\n",
        "        x_mixed = self.complex_linear(x_gated, self.mix_real, self.mix_imag)\n",
        "        x_act = self.activation(x_mixed)\n",
        "        out = self.complex_linear(x_act, self.out_real, self.out_imag)\n",
        "        return self.dropout(out) + residual\n",
        "\n",
        "# --- ENCODER MUST BE DEFINED AFTER LAYER ---\n",
        "class PRISMEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([PRISMLayer(d_model, max_len, dropout) for _ in range(num_layers)])\n",
        "        self.final_norm = PhasePreservingLayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return self.final_norm(x)\n",
        "\n",
        "# --- THE CORRECT BRIDGE (Cartesian) ---\n",
        "class ComplexToRealBridge(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model * 2, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x_complex):\n",
        "        if x_complex is None: raise ValueError(\"Bridge None\")\n",
        "        cat = torch.cat([x_complex.real, x_complex.imag], dim=-1)\n",
        "        return self.norm(self.proj(cat))\n",
        "\n",
        "class PRISMHybrid_RoPE(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_refining_layers, num_decoder_layers,\n",
        "                 num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        # 1. Initialize HarmonicEmbedding (It now holds the Master Embedding)\n",
        "        self.harmonic_embedding = HarmonicEmbedding(vocab_size, d_model)\n",
        "\n",
        "        # 2. POINTER MAGIC: Point the decoder's embedding to the Encoder's Master\n",
        "        self.tgt_embedding = self.harmonic_embedding.raw_embedding\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if num_encoder_layers > 0:\n",
        "            self.prism_encoder = PRISMEncoder(num_encoder_layers, d_model, max_length, dropout)\n",
        "        else:\n",
        "            self.prism_encoder = None\n",
        "\n",
        "        self.bridge = ComplexToRealBridge(d_model)\n",
        "\n",
        "        if num_refining_layers > 0:\n",
        "            refining_layer = nn.TransformerEncoderLayer(\n",
        "                d_model, num_heads, dff, dropout,\n",
        "                batch_first=True, norm_first=True\n",
        "            )\n",
        "            self.reasoning_encoder = nn.TransformerEncoder(refining_layer, num_layers=num_refining_layers)\n",
        "        else:\n",
        "            self.reasoning_encoder = None\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            dim = d_model, depth = num_decoder_layers, heads = num_heads, attn_dim_head = d_model // num_heads,\n",
        "            ff_mult = dff / d_model, rotary_pos_emb = True, cross_attend = True, attn_flash = True,\n",
        "            attn_dropout = dropout, ff_dropout = dropout, use_rmsnorm = True\n",
        "        )\n",
        "\n",
        "        # 3. Output Projection\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # 4. WEIGHT TYING (The \"Press Attention Later\" requirement)\n",
        "        # We tie the Output Head weights to the Master Input Embedding weights\n",
        "        self.final_linear.weight = self.tgt_embedding.weight\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sz=tgt.size(1), device=src.device, dtype=torch.bool)\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_pad, mem_pad, tgt_mask):\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "        if src_mask is not None:\n",
        "            src_harmonic = src_harmonic.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        if self.prism_encoder is not None:\n",
        "            if self.training:\n",
        "                src_harmonic.requires_grad_(True)\n",
        "                encoded_complex = torch.utils.checkpoint.checkpoint(\n",
        "                    self.prism_encoder.forward, # Safest\n",
        "                    src_harmonic, src_mask, use_reentrant=False\n",
        "                )\n",
        "            else:\n",
        "                encoded_complex = self.prism_encoder(src_harmonic, src_mask)\n",
        "        else:\n",
        "            encoded_complex = src_harmonic\n",
        "\n",
        "        coarse_memory = self.bridge(encoded_complex)\n",
        "        if self.reasoning_encoder is not None:\n",
        "            refined_memory = self.reasoning_encoder(coarse_memory, src_key_padding_mask=mem_pad)\n",
        "        else:\n",
        "            refined_memory = coarse_memory\n",
        "\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.dropout(tgt_emb)\n",
        "        context_mask = ~mem_pad if mem_pad is not None else None\n",
        "        decoder_mask = ~tgt_pad if tgt_pad is not None else None\n",
        "\n",
        "        if self.training:\n",
        "            tgt_emb.requires_grad_(True)\n",
        "            output = torch.utils.checkpoint.checkpoint(\n",
        "                self.decoder, tgt_emb, context=refined_memory, mask=decoder_mask, context_mask=context_mask, use_reentrant=False\n",
        "            )\n",
        "        else:\n",
        "            output = self.decoder(tgt_emb, context=refined_memory, mask=decoder_mask, context_mask=context_mask)\n",
        "\n",
        "        return self.final_linear(output)\n",
        "\n",
        "    # ... (generate function remains the same) ...\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src, max_length, num_beams=5):\n",
        "        self.eval()\n",
        "        src_mask = (src == tokenizer.pad_token_id)\n",
        "        context_mask = ~src_mask\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "        if src_mask is not None:\n",
        "            src_harmonic = src_harmonic.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        if self.prism_encoder is not None:\n",
        "            encoded_complex = self.prism_encoder(src_harmonic, src_mask)\n",
        "        else:\n",
        "            encoded_complex = src_harmonic\n",
        "\n",
        "        coarse_memory = self.bridge(encoded_complex)\n",
        "\n",
        "        if self.reasoning_encoder is not None:\n",
        "            memory = self.reasoning_encoder(coarse_memory, src_key_padding_mask=src_mask)\n",
        "        else:\n",
        "            memory = coarse_memory\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        context_mask = context_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        beams = torch.full((batch_size * num_beams, 1), tokenizer.pad_token_id, dtype=torch.long, device=src.device)\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_emb = self.tgt_embedding(beams) * math.sqrt(self.d_model)\n",
        "            tgt_emb = self.dropout(tgt_emb)\n",
        "\n",
        "            # Decoder\n",
        "            decoder_output = self.decoder(tgt_emb, context=memory, context_mask=context_mask)\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # Masking\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "\n",
        "            # --- BEAM SEARCH LOGIC FIX ---\n",
        "            if _ == 0:\n",
        "                # First Step: Expand from the first beam only (since all are identical start tokens)\n",
        "                # Reshape to (batch, beams, vocab)\n",
        "                total = (beam_scores.unsqueeze(1) + log_probs).view(batch_size, num_beams, -1)\n",
        "                # Mask out all beams except the first one (-inf)\n",
        "                total[:, 1:, :] = -torch.inf\n",
        "                # Flatten back to (batch, beams*vocab) to pick top k\n",
        "                total = total.view(batch_size, -1)\n",
        "            else:\n",
        "                # Subsequent Steps: Standard Flatten\n",
        "                total = (beam_scores.unsqueeze(1) + log_probs).view(batch_size, -1)\n",
        "\n",
        "            top_scores, top_indices = torch.topk(total, k=num_beams, dim=1)\n",
        "\n",
        "            beam_indices = top_indices // log_probs.shape[-1]\n",
        "            token_indices = top_indices % log_probs.shape[-1]\n",
        "\n",
        "            # Now dimensions match: (batch_size, 1) + (batch_size, k)\n",
        "            effective = (torch.arange(batch_size, device=src.device).unsqueeze(1) * num_beams + beam_indices).view(-1)\n",
        "            beams = torch.cat([beams[effective], token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        best_beams = final_beams[:, 0, :]\n",
        "        self.train()\n",
        "        return best_beams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFiIvRiyDg8K"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.text import SacreBLEUScore\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    # Use SacreBLEUScore (defaults to '13a' tokenizer, the WMT standard)\n",
        "    metric = SacreBLEUScore().to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Use no_grad to save memory and speed up validation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Generate predictions\n",
        "            generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "            # Decode predictions\n",
        "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Decode labels (Fixing -100 padding)\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Update Metric\n",
        "            # SacreBLEU expects references as a list of lists: [[ref1], [ref2], ...]\n",
        "            formatted_refs = [[ref] for ref in ref_texts]\n",
        "            metric.update(pred_texts, formatted_refs)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Compute returns a tensor, .item() converts it to a standard python float\n",
        "    return metric.compute().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij7ZQQ7bEN2R"
      },
      "outputs": [],
      "source": [
        "def analyze_prism_params(model):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ðŸ“Š MODEL PARAMETER ANALYSIS: {model.__class__.__name__}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # PyTorch's model.parameters() automatically handles deduplication of shared weights.\n",
        "    # It will count the shared embedding matrix exactly ONCE.\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # --- 1. COMPONENT BREAKDOWN ---\n",
        "\n",
        "    # A. Embeddings & Adapter\n",
        "    # This includes the shared raw_embedding + the encoder adapter\n",
        "    emb_params = sum(p.numel() for p in model.harmonic_embedding.parameters())\n",
        "\n",
        "    # B. PRISM Encoder\n",
        "    if model.prism_encoder is not None:\n",
        "        enc_params = sum(p.numel() for p in model.prism_encoder.parameters())\n",
        "    else:\n",
        "        enc_params = 0\n",
        "\n",
        "    # C. Bridge\n",
        "    bridge_params = sum(p.numel() for p in model.bridge.parameters())\n",
        "\n",
        "    # D. Reasoning/Refining Encoder (if you added it)\n",
        "    if model.reasoning_encoder is not None:\n",
        "        reasoning_params = sum(p.numel() for p in model.reasoning_encoder.parameters())\n",
        "    else:\n",
        "        reasoning_params = 0\n",
        "\n",
        "    # E. Transformer Decoder\n",
        "    dec_params = sum(p.numel() for p in model.decoder.parameters())\n",
        "\n",
        "    # F. Output Head (Bias Only)\n",
        "    # The weight is shared (counted in A), but the bias is unique to the head.\n",
        "    head_bias_params = model.final_linear.bias.numel() if model.final_linear.bias is not None else 0\n",
        "\n",
        "    # --- 2. WEIGHT TYING CHECK ---\n",
        "    # We check memory addresses (id) to ensure they point to the exact same tensor object\n",
        "    tie_1 = (model.tgt_embedding.weight is model.harmonic_embedding.raw_embedding.weight)\n",
        "    tie_2 = (model.final_linear.weight is model.harmonic_embedding.raw_embedding.weight)\n",
        "\n",
        "    print(f\"{'COMPONENT':<25} | {'PARAMS':<15} | {'% OF TOTAL':<10}\")\n",
        "    print(\"-\" * 56)\n",
        "    print(f\"{'Embeddings (+Adapter)':<25} | {emb_params:<15,} | {emb_params/total_params:.1%}\")\n",
        "    print(f\"{'PRISM Encoder':<25} | {enc_params:<15,} | {enc_params/total_params:.1%}\")\n",
        "    print(f\"{'Bridge':<25} | {bridge_params:<15,} | {bridge_params/total_params:.1%}\")\n",
        "    if reasoning_params > 0:\n",
        "        print(f\"{'Reasoning Encoder':<25} | {reasoning_params:<15,} | {reasoning_params/total_params:.1%}\")\n",
        "    print(f\"{'Transformer Decoder':<25} | {dec_params:<15,} | {dec_params/total_params:.1%}\")\n",
        "    print(f\"{'Head (Bias)':<25} | {head_bias_params:<15,} | {head_bias_params/total_params:.1%}\")\n",
        "    print(\"-\" * 56)\n",
        "    print(f\"{'TOTAL TRAINABLE':<25} | {trainable_params:<15,} | 100.0%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nðŸ”— WEIGHT TYING VERIFICATION\")\n",
        "    print(f\"Enc Input == Dec Input?  {'âœ… YES' if tie_1 else 'âŒ NO (Memory Waste!)'}\")\n",
        "    print(f\"Enc Input == Output Head? {'âœ… YES' if tie_2 else 'âŒ NO (Memory Waste!)'}\")\n",
        "\n",
        "    if tie_1 and tie_2:\n",
        "        # Calculate strict matrix savings (Vocab * D_model * 2 instances saved)\n",
        "        # 1 saved instance from Dec Input, 1 saved instance from Output Head\n",
        "        vocab_size = model.harmonic_embedding.raw_embedding.num_embeddings\n",
        "        d_model = model.harmonic_embedding.raw_embedding.embedding_dim\n",
        "        saved_count = (vocab_size * d_model) * 2\n",
        "\n",
        "        print(f\"\\nðŸ’¡ Efficiency Note: Weight tying saved {saved_count:,} params.\")\n",
        "        print(f\"   Without tying, model size would be: {(total_params + saved_count):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8M_uQNvKolVF"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 5. EXECUTION (FIXED INITIALIZATION)\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    experiment_name = f\"PRISM_Hybrid_RoPE_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "    logger.info(\"Initializing PRISM...\")\n",
        "\n",
        "    model = PRISMHybrid_RoPE(\n",
        "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "        num_refining_layers=NUM_REFINING_LAYERS,\n",
        "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "        num_heads=NUM_HEADS,\n",
        "        d_model=D_MODEL,\n",
        "        dff=D_FF,\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # 1. Generic Init\n",
        "    def init_weights_PRISM(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            std = 1.0 / math.sqrt(D_MODEL)\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=std)\n",
        "            if m.padding_idx is not None: nn.init.constant_(m.weight[m.padding_idx], 0.0)\n",
        "        elif hasattr(m, 'global_filter'):\n",
        "            nn.init.normal_(m.global_filter, mean=0.0, std=0.02)\n",
        "\n",
        "    model.apply(init_weights_PRISM)\n",
        "\n",
        "    # 2. THE FIX: Re-Initialize the Shared Embeddings CORRECTLY\n",
        "    # We overwrite the Xavier noise from the final_linear back to Normal\n",
        "    logger.info(\"ðŸ”§ Fixing Shared Embedding Initialization...\")\n",
        "    std = 1.0 / math.sqrt(D_MODEL)\n",
        "    nn.init.normal_(model.harmonic_embedding.raw_embedding.weight, mean=0.0, std=std)\n",
        "    if model.harmonic_embedding.raw_embedding.padding_idx is not None:\n",
        "        nn.init.constant_(model.harmonic_embedding.raw_embedding.weight[model.harmonic_embedding.raw_embedding.padding_idx], 0.0)\n",
        "\n",
        "    logger.info(\"âœ… Initialization Complete.\")\n",
        "    analyze_prism_params(model)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=PEAK_LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, TARGET_TRAINING_STEPS)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=LABEL_SMOOTHING_EPSILON)\n",
        "\n",
        "    # RESUME LOGIC (Optional - Leave empty to start fresh)\n",
        "    RESUME_CHECKPOINT_PATH = \"\"\n",
        "    if RESUME_CHECKPOINT_PATH and os.path.exists(RESUME_CHECKPOINT_PATH):\n",
        "        logger.info(f\"ðŸ”„ Loading checkpoint from {RESUME_CHECKPOINT_PATH}\")\n",
        "        checkpoint = torch.load(RESUME_CHECKPOINT_PATH, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        global_step = checkpoint['step']\n",
        "        best_bleu = checkpoint.get('best_bleu', 0.0)\n",
        "        logger.info(f\"âœ… Resumed Step: {global_step}, Best BLEU: {best_bleu}\")\n",
        "    else:\n",
        "        global_step = 0\n",
        "        best_bleu = 0.0\n",
        "\n",
        "    logger.info(f\"STARTING MARATHON ({TARGET_TRAINING_STEPS} steps)\")\n",
        "    model.train()\n",
        "    progress = tqdm(initial=global_step, total=TARGET_TRAINING_STEPS)\n",
        "\n",
        "    while global_step < TARGET_TRAINING_STEPS:\n",
        "        for batch in train_dataloader:\n",
        "            if global_step >= TARGET_TRAINING_STEPS: break\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "            dec_in = torch.cat([torch.full((labels.size(0), 1), tokenizer.pad_token_id, device=device), labels[:, :-1]], dim=1)\n",
        "            dec_in[dec_in == -100] = tokenizer.pad_token_id\n",
        "\n",
        "            src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, dec_in)\n",
        "            tgt_pad[:, 0] = False\n",
        "\n",
        "            # Checkpointing (Matches your \"OOM at 6k\" working version)\n",
        "            out = model(input_ids, dec_in, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "            loss = loss_fn(out.view(-1, VOCAB_SIZE), labels.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            progress.update(1)\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                writer.add_scalar('train/loss', loss.item(), global_step)\n",
        "                writer.add_scalar('train/grad_norm', grad_norm.item(), global_step)\n",
        "                progress.set_postfix(loss=loss.item(), gnorm=f\"{grad_norm.item():.2f}\")\n",
        "\n",
        "            if global_step in VALIDATION_SCHEDULE:\n",
        "                logger.info(f\"Validating at step {global_step}...\")\n",
        "                current_bleu = evaluate(model, val_dataloader, device)\n",
        "                writer.add_scalar('val/bleu', current_bleu, global_step)\n",
        "                logger.info(f\"Step {global_step} | BLEU: {current_bleu:.4f}\")\n",
        "\n",
        "                ckpt = {'step': global_step, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(), 'best_bleu': best_bleu}\n",
        "                torch.save(ckpt, os.path.join(SAVE_DIR, \"last.pt\"))\n",
        "                if current_bleu > best_bleu:\n",
        "                    best_bleu = current_bleu\n",
        "                    ckpt['best_bleu'] = best_bleu\n",
        "                    torch.save(ckpt, os.path.join(SAVE_DIR, \"best_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xsWfDkeWp5-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "import torchmetrics\n",
        "import numpy\n",
        "import pkg_resources\n",
        "\n",
        "def log_environment_separate(log_dir):\n",
        "    # Define the separate file path\n",
        "    meta_file = os.path.join(log_dir, \"system_metadata.txt\")\n",
        "\n",
        "    with open(meta_file, \"w\") as f:\n",
        "        # --- PART 1: SUMMARY ---\n",
        "        f.write(\"=\"*40 + \"\\n\")\n",
        "        f.write(\"CORE ENVIRONMENT SUMMARY\\n\")\n",
        "        f.write(\"=\"*40 + \"\\n\")\n",
        "        f.write(f\"Python:       {sys.version.split()[0]}\\n\")\n",
        "        f.write(f\"PyTorch:      {torch.__version__}\\n\")\n",
        "        f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
        "        f.write(f\"Datasets:     {datasets.__version__}\\n\")\n",
        "        f.write(f\"TorchMetrics: {torchmetrics.__version__}\\n\")\n",
        "        f.write(f\"NumPy:        {numpy.__version__}\\n\")\n",
        "\n",
        "        try:\n",
        "            import sacrebleu\n",
        "            f.write(f\"SacreBLEU:    {sacrebleu.__version__}\\n\")\n",
        "        except ImportError:\n",
        "            f.write(\"SacreBLEU:    Not Installed\\n\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            f.write(f\"GPU Name:     {torch.cuda.get_device_name(0)}\\n\")\n",
        "            f.write(f\"CUDA Ver:     {torch.version.cuda}\\n\")\n",
        "            f.write(f\"Capability:   {torch.cuda.get_device_capability(0)}\\n\")\n",
        "        else:\n",
        "            f.write(\"GPU:          None (CPU Only)\\n\")\n",
        "\n",
        "        # --- PART 2: FULL FREEZE ---\n",
        "        f.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "        f.write(\"FULL LIBRARY DEPENDENCIES (PIP FREEZE)\\n\")\n",
        "        f.write(\"=\"*40 + \"\\n\")\n",
        "\n",
        "        installed_packages = {d.project_name: d.version for d in pkg_resources.working_set}\n",
        "        for package, version in sorted(installed_packages.items()):\n",
        "            f.write(f\"{package}=={version}\\n\")\n",
        "\n",
        "    print(f\"âœ… Environment details saved SEPARATELY to: {meta_file}\")\n",
        "\n",
        "# Execute\n",
        "# Assumes CURRENT_RUN_DIR is defined from your config\n",
        "log_environment_separate(CURRENT_RUN_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7bFIVCLfCdT"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "mount_file_id": "1jb8bK7FUPW6u0TArX5RlakQwbX8yR2T2",
      "authorship_tag": "ABX9TyPafM6lVNBDoK08SNrvDoYn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}