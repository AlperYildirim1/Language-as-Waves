{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/Shimmer_PRISM_train_hybrid_RoPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2CVny1CrxQc"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics sacrebleu x-transformers\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ==============================================================================\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import math, sys, logging, datetime, json, random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from typing import List\n",
        "\n",
        "# --- Hardware Speedups ---\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "# --- Data & Task Size ---\n",
        "MAX_LENGTH = 128\n",
        "MODEL_CHOICE = \"Shimmer\"\n",
        "\n",
        "# --- Model Architecture Config ---\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "NUM_ENCODER_LAYERS = 6  # PRISM LAYERS\n",
        "NUM_REFINING_LAYERS = 0 #\n",
        "NUM_DECODER_LAYERS = 6\n",
        "\n",
        "# --- Training Config ---\n",
        "TARGET_TRAINING_STEPS = 50000\n",
        "VALIDATION_SCHEDULE = [\n",
        "    2000, 4000, 5000, 7500, 10000, 15000, 20000,\n",
        "    25000, 30000, 35000, 42500, 50000, #57500, 65000, 72500, 90000, 100000\n",
        "]\n",
        "\n",
        "PEAK_LEARNING_RATE = 8e-4\n",
        "WARMUP_STEPS = 600\n",
        "WEIGHT_DECAY = 0.01\n",
        "LABEL_SMOOTHING_EPSILON = 0.1\n",
        "\n",
        "# --- Paths ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/PRISM\"\n",
        "PREBATCHED_REPO_ID = \"Yujivus/wmt14-de-en-prebatched-w4\"\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VuaI43WDGoA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 2. IMPORTS & SETUP\n",
        "# ==============================================================================\n",
        "from x_transformers import Decoder\n",
        "\n",
        "def set_seed(seed_value=116):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Logging Setup ---\n",
        "experiment_name = f\"{MODEL_CHOICE}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "CURRENT_RUN_DIR = os.path.join(DRIVE_BASE_PATH, experiment_name)\n",
        "SAVE_DIR = os.path.join(CURRENT_RUN_DIR, \"models\")\n",
        "LOG_DIR_TENSORBOARD = os.path.join(CURRENT_RUN_DIR, \"tensorboard_logs\")\n",
        "LOG_FILE_TXT = os.path.join(CURRENT_RUN_DIR, \"run_log.txt\")\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR_TENSORBOARD, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(message)s',\n",
        "    handlers=[logging.FileHandler(LOG_FILE_TXT), logging.StreamHandler(sys.stdout)],\n",
        "    force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "writer = SummaryWriter(LOG_DIR_TENSORBOARD)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DATA LOADING\n",
        "# ==============================================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "class PreBatchedCollator:\n",
        "    def __init__(self, original_dataset_split):\n",
        "        self.original_dataset = original_dataset_split\n",
        "    def __call__(self, features: List[dict]) -> dict:\n",
        "        batch_indices = features[0]['batch_indices']\n",
        "        dict_of_lists = self.original_dataset[batch_indices]\n",
        "        list_of_dicts = []\n",
        "        keys = dict_of_lists.keys()\n",
        "        num_samples = len(dict_of_lists['input_ids'])\n",
        "        for i in range(num_samples):\n",
        "            list_of_dicts.append({key: dict_of_lists[key][i] for key in keys})\n",
        "        return standard_collator(list_of_dicts)\n",
        "\n",
        "logger.info(f\"Loading datasets...\")\n",
        "prebatched_datasets = load_dataset(PREBATCHED_REPO_ID)\n",
        "original_datasets = load_dataset(ORIGINAL_BUCKETED_REPO_ID)\n",
        "train_collator = PreBatchedCollator(original_datasets[\"train\"])\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    prebatched_datasets[\"train\"], batch_size=1, shuffle=True,\n",
        "    collate_fn=train_collator, num_workers=2, pin_memory=True, prefetch_factor=2\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    original_datasets[\"validation\"], batch_size=64,\n",
        "    collate_fn=standard_collator, num_workers=2\n",
        ")\n",
        "# ==============================================================================\n",
        "# 4. PRISM ARCHITECTURE (FIXED: COMPLEX DROPOUT & PADDING)\n",
        "# ==============================================================================\n",
        "\n",
        "class ComplexDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    FIX: Standard nn.Dropout doesn't work on ComplexFloat.\n",
        "    This module generates a mask based on the shape and applies it to both\n",
        "    Real and Imaginary parts identically to preserve Phase.\n",
        "    \"\"\"\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, z):\n",
        "        if not self.training or self.p == 0.0:\n",
        "            return z\n",
        "\n",
        "        # Generate mask using F.dropout on a ones tensor of the same shape (Real part)\n",
        "        # F.dropout handles the scaling (1 / 1-p) automatically\n",
        "        mask = torch.ones_like(z.real)\n",
        "        mask = F.dropout(mask, self.p, self.training, inplace=False)\n",
        "\n",
        "        # Apply mask to the complex tensor\n",
        "        return z * mask\n",
        "\n",
        "class PhasePreservingLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model, eps=eps)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mag = torch.abs(x)\n",
        "        mag_norm = self.layernorm(mag)\n",
        "        # Avoid division by zero\n",
        "        return mag_norm.to(x.dtype) * (x / (mag + self.eps))\n",
        "\n",
        "class HarmonicEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, max_period=10000.0):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # 1. Learnable Real and Imaginary parts (Cartesian coordinates)\n",
        "        # This allows learning both Amplitude AND Intrinsic Phase implicitly\n",
        "        self.complex_embedding = nn.Embedding(num_embeddings, embedding_dim * 2)\n",
        "\n",
        "        # Frequencies (Fixed)\n",
        "        freqs = torch.exp(torch.arange(0, embedding_dim, dtype=torch.float32) * -(math.log(max_period) / embedding_dim))\n",
        "        self.register_buffer('freqs', freqs)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # A. Get Learnable Content (Mag + Intrinsic Phase)\n",
        "        # Shape: [Batch, Seq, Dim * 2]\n",
        "        raw_embeds = self.complex_embedding(input_ids)\n",
        "\n",
        "        # Split into Real/Imag\n",
        "        real = raw_embeds[..., :self.embedding_dim]\n",
        "        imag = raw_embeds[..., self.embedding_dim:]\n",
        "\n",
        "        # Convert to Complex Tensor\n",
        "        # This Z already has Amplitude AND Intrinsic Phase\n",
        "        content_z = torch.complex(real, imag)\n",
        "\n",
        "        # B. Apply Positional Rotation (The \"Clock\")\n",
        "        seq_len = input_ids.shape[1]\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).float()\n",
        "        angles = torch.outer(positions, self.freqs)\n",
        "\n",
        "        # Create Rotation (Phase Shift)\n",
        "        # e^(i * theta)\n",
        "        pos_rotation = torch.polar(torch.ones_like(angles), angles).unsqueeze(0)\n",
        "\n",
        "        # C. Rotate the Content\n",
        "        # Z_final = Z_content * e^(i * pos)\n",
        "        return content_z * pos_rotation\n",
        "\n",
        "class PRISMEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([PRISMLayer(d_model, max_len, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.final_norm = PhasePreservingLayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        # Apply Final Norm\n",
        "        return self.final_norm(x)\n",
        "\n",
        "class ModReLU(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.b = nn.Parameter(torch.zeros(features))\n",
        "    def forward(self, z):\n",
        "        mag = torch.abs(z)\n",
        "        new_mag = F.relu(mag + self.b)\n",
        "        phase = z / (mag + 1e-6)\n",
        "        return new_mag * phase\n",
        "\n",
        "class PRISMLayer(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.filter_len = max_len\n",
        "\n",
        "        # --- REMOVED GATING PARAMS ---\n",
        "        # self.pre_gate = nn.Linear(d_model * 2, d_model)\n",
        "\n",
        "        # Global Filter\n",
        "        self.global_filter = nn.Parameter(torch.randn(d_model, max_len, dtype=torch.cfloat) * 0.02)\n",
        "\n",
        "        # Mixing\n",
        "        self.mix_real = nn.Linear(d_model, d_model)\n",
        "        self.mix_imag = nn.Linear(d_model, d_model)\n",
        "        self.out_real = nn.Linear(d_model, d_model)\n",
        "        self.out_imag = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.activation = ModReLU(d_model)\n",
        "        self.norm = PhasePreservingLayerNorm(d_model)\n",
        "        self.dropout = ComplexDropout(dropout)\n",
        "\n",
        "    def complex_linear(self, x, l_real, l_imag):\n",
        "        r, i = x.real, x.imag\n",
        "        new_r = l_real(r) - l_imag(i)\n",
        "        new_i = l_real(i) + l_imag(r)\n",
        "        return torch.complex(new_r, new_i)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        residual = x\n",
        "        x_norm = self.norm(x)\n",
        "\n",
        "        if src_mask is not None:\n",
        "            mask_expanded = src_mask.unsqueeze(-1)\n",
        "            x_norm = x_norm.masked_fill(mask_expanded, 0.0)\n",
        "\n",
        "        # --- REMOVED GATING LOGIC ---\n",
        "        # Pass x_norm directly to FFT\n",
        "        x_gated = x_norm\n",
        "\n",
        "        # B. FFT Resonance\n",
        "        B, L, D = x_gated.shape\n",
        "        x_freq = torch.fft.fft(x_gated, n=self.filter_len, dim=1)\n",
        "        filter_transposed = self.global_filter.transpose(-1, -2)\n",
        "        x_filtered = x_freq * filter_transposed\n",
        "        x_time = torch.fft.ifft(x_filtered, n=self.filter_len, dim=1)\n",
        "        x_time = x_time[:, :L, :]\n",
        "\n",
        "        # C. Mix & Activate\n",
        "        x_mixed = self.complex_linear(x_time, self.mix_real, self.mix_imag)\n",
        "        x_act = self.activation(x_mixed)\n",
        "        out = self.complex_linear(x_act, self.out_real, self.out_imag)\n",
        "\n",
        "        return self.dropout(out) + residual\n",
        "\n",
        "class ComplexToRealBridge(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model * 2, d_model)\n",
        "    def forward(self, x_complex):\n",
        "        cat = torch.cat([x_complex.real, x_complex.imag], dim=-1)\n",
        "        return self.proj(cat)\n",
        "\n",
        "class PRISMHybrid_RoPE(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_refining_layers, num_decoder_layers,\n",
        "                 num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 1. Embeddings\n",
        "        self.harmonic_embedding = HarmonicEmbedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 2. Harmonic Body (PRISM Encoder)\n",
        "        if num_encoder_layers > 0:\n",
        "            self.prism_encoder = PRISMEncoder(num_encoder_layers, d_model, max_length, dropout)\n",
        "        else:\n",
        "            self.prism_encoder = None\n",
        "\n",
        "        # 3. The Bridge\n",
        "        self.bridge = ComplexToRealBridge(d_model)\n",
        "\n",
        "        # 4. Refining Encoder\n",
        "        if num_refining_layers > 0:\n",
        "            refining_layer = nn.TransformerEncoderLayer(\n",
        "                d_model, num_heads, dff, dropout,\n",
        "                batch_first=True, norm_first=True\n",
        "            )\n",
        "            self.reasoning_encoder = nn.TransformerEncoder(refining_layer, num_layers=num_refining_layers)\n",
        "        else:\n",
        "            self.reasoning_encoder = None\n",
        "\n",
        "        # 5. Decoder (x-transformers)\n",
        "        self.decoder = Decoder(\n",
        "            dim = d_model,\n",
        "            depth = num_decoder_layers,\n",
        "            heads = num_heads,\n",
        "            attn_dim_head = d_model // num_heads,\n",
        "            ff_mult = dff / d_model,\n",
        "            rotary_pos_emb = True,\n",
        "            cross_attend = True,\n",
        "            attn_flash = True,\n",
        "            attn_dropout = dropout,\n",
        "            ff_dropout = dropout,\n",
        "            use_rmsnorm = True\n",
        "        )\n",
        "\n",
        "        # 6. Output Head\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.final_linear.weight = self.tgt_embedding.weight\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            sz=tgt.size(1), device=src.device, dtype=torch.bool\n",
        "        )\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_pad, mem_pad, tgt_mask):\n",
        "        # A. Harmonic Phase\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "        if src_mask is not None:\n",
        "            src_harmonic = src_harmonic.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        # PRISM Encoder Pass\n",
        "        if self.prism_encoder is not None:\n",
        "            if self.training:\n",
        "                src_harmonic.requires_grad_(True)\n",
        "                encoded_complex = torch.utils.checkpoint.checkpoint(\n",
        "                    self.prism_encoder, src_harmonic, src_mask, use_reentrant=False\n",
        "                )\n",
        "            else:\n",
        "                encoded_complex = self.prism_encoder(src_harmonic, src_mask)\n",
        "        else:\n",
        "            encoded_complex = src_harmonic\n",
        "\n",
        "        # B. The Bridge\n",
        "        coarse_memory = self.bridge(encoded_complex)\n",
        "\n",
        "        # C. Refining Phase\n",
        "        if self.reasoning_encoder is not None:\n",
        "            refined_memory = self.reasoning_encoder(coarse_memory, src_key_padding_mask=mem_pad)\n",
        "        else:\n",
        "            refined_memory = coarse_memory\n",
        "\n",
        "        # D. Decoder Prep\n",
        "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.dropout(tgt_emb)\n",
        "        context_mask = ~mem_pad if mem_pad is not None else None\n",
        "        decoder_mask = ~tgt_pad if tgt_pad is not None else None\n",
        "\n",
        "        # E. Decoder Pass (Checkpointing)\n",
        "        if self.training:\n",
        "            tgt_emb.requires_grad_(True)\n",
        "            output = torch.utils.checkpoint.checkpoint(\n",
        "                self.decoder,\n",
        "                tgt_emb,\n",
        "                context=refined_memory,\n",
        "                mask=decoder_mask,\n",
        "                context_mask=context_mask,\n",
        "                use_reentrant=False\n",
        "            )\n",
        "        else:\n",
        "            output = self.decoder(\n",
        "                tgt_emb,\n",
        "                context=refined_memory,\n",
        "                mask=decoder_mask,\n",
        "                context_mask=context_mask\n",
        "            )\n",
        "\n",
        "        return self.final_linear(output)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src, max_length, num_beams=5):\n",
        "        self.eval()\n",
        "        src_mask = (src == tokenizer.pad_token_id)\n",
        "        context_mask = ~src_mask\n",
        "        src_harmonic = self.harmonic_embedding(src)\n",
        "        if src_mask is not None:\n",
        "            src_harmonic = src_harmonic.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
        "\n",
        "        if self.prism_encoder is not None:\n",
        "            encoded_complex = self.prism_encoder(src_harmonic, src_mask)\n",
        "        else:\n",
        "            encoded_complex = src_harmonic\n",
        "\n",
        "        coarse_memory = self.bridge(encoded_complex)\n",
        "\n",
        "        if self.reasoning_encoder is not None:\n",
        "            memory = self.reasoning_encoder(coarse_memory, src_key_padding_mask=src_mask)\n",
        "        else:\n",
        "            memory = coarse_memory\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        context_mask = context_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        beams = torch.full((batch_size * num_beams, 1), tokenizer.pad_token_id, dtype=torch.long, device=src.device)\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_emb = self.tgt_embedding(beams) * math.sqrt(self.d_model)\n",
        "            tgt_emb = self.dropout(tgt_emb)\n",
        "\n",
        "            # Decoder\n",
        "            decoder_output = self.decoder(tgt_emb, context=memory, context_mask=context_mask)\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # Masking\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "\n",
        "            # --- BEAM SEARCH LOGIC FIX ---\n",
        "            if _ == 0:\n",
        "                # First Step: Expand from the first beam only (since all are identical start tokens)\n",
        "                # Reshape to (batch, beams, vocab)\n",
        "                total = (beam_scores.unsqueeze(1) + log_probs).view(batch_size, num_beams, -1)\n",
        "                # Mask out all beams except the first one (-inf)\n",
        "                total[:, 1:, :] = -torch.inf\n",
        "                # Flatten back to (batch, beams*vocab) to pick top k\n",
        "                total = total.view(batch_size, -1)\n",
        "            else:\n",
        "                # Subsequent Steps: Standard Flatten\n",
        "                total = (beam_scores.unsqueeze(1) + log_probs).view(batch_size, -1)\n",
        "\n",
        "            top_scores, top_indices = torch.topk(total, k=num_beams, dim=1)\n",
        "\n",
        "            beam_indices = top_indices // log_probs.shape[-1]\n",
        "            token_indices = top_indices % log_probs.shape[-1]\n",
        "\n",
        "            # Now dimensions match: (batch_size, 1) + (batch_size, k)\n",
        "            effective = (torch.arange(batch_size, device=src.device).unsqueeze(1) * num_beams + beam_indices).view(-1)\n",
        "            beams = torch.cat([beams[effective], token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        best_beams = final_beams[:, 0, :]\n",
        "        self.train()\n",
        "        return best_beams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFiIvRiyDg8K"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.text import SacreBLEUScore\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    # Use SacreBLEUScore (defaults to '13a' tokenizer, the WMT standard)\n",
        "    metric = SacreBLEUScore().to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Use no_grad to save memory and speed up validation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Generate predictions\n",
        "            generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "            # Decode predictions\n",
        "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Decode labels (Fixing -100 padding)\n",
        "            labels[labels == -100] = tokenizer.pad_token_id\n",
        "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Update Metric\n",
        "            # SacreBLEU expects references as a list of lists: [[ref1], [ref2], ...]\n",
        "            formatted_refs = [[ref] for ref in ref_texts]\n",
        "            metric.update(pred_texts, formatted_refs)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Compute returns a tensor, .item() converts it to a standard python float\n",
        "    return metric.compute().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M_uQNvKolVF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 5. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    experiment_name = f\"PRISM_Hybrid_RoPE_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "    config_state = {\"model\": MODEL_CHOICE, \"d_model\": D_MODEL, \"layers\": NUM_ENCODER_LAYERS,\n",
        "                    \"lr\": PEAK_LEARNING_RATE, \"seed\": 116}\n",
        "\n",
        "    logger.info(\"Initializing PRISM...\")\n",
        "\n",
        "    model = PRISMHybrid_RoPE(\n",
        "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "        num_refining_layers=NUM_REFINING_LAYERS,\n",
        "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "        num_heads=NUM_HEADS,\n",
        "        d_model=D_MODEL,\n",
        "        dff=D_FF,\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # FIX: Robust Initialization that respects the Gate Bias\n",
        "    def init_weights_PRISM(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # Check if this is the Spectral Gate (marked with tag)\n",
        "            if hasattr(m, 'is_gate') and m.is_gate:\n",
        "                return # Skip initialization for the gate (it's already 2.0)\n",
        "\n",
        "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "            if m.bias is not None:\n",
        "                nn.init.uniform_(m.bias, -0.1, 0.1)\n",
        "\n",
        "    model.apply(init_weights_PRISM)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=PEAK_LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, TARGET_TRAINING_STEPS)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=LABEL_SMOOTHING_EPSILON)\n",
        "\n",
        "    logger.info(f\"STARTING MARATHON ({TARGET_TRAINING_STEPS} steps)\")\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    best_bleu = 0.0\n",
        "    progress = tqdm(total=TARGET_TRAINING_STEPS)\n",
        "\n",
        "    while global_step < TARGET_TRAINING_STEPS:\n",
        "        for batch in train_dataloader:\n",
        "            if global_step >= TARGET_TRAINING_STEPS: break\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "            dec_in = torch.cat([torch.full((labels.size(0), 1), tokenizer.pad_token_id, device=device), labels[:, :-1]], dim=1)\n",
        "            dec_in[dec_in == -100] = tokenizer.pad_token_id\n",
        "\n",
        "            src_mask, tgt_pad, mem_pad, tgt_mask = model.create_masks(input_ids, dec_in)\n",
        "            tgt_pad[:, 0] = False\n",
        "\n",
        "            out = model(input_ids, dec_in, src_mask, tgt_pad, mem_pad, tgt_mask)\n",
        "            loss = loss_fn(out.view(-1, VOCAB_SIZE), labels.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # --- MODIFICATION START ---\n",
        "            # clip_grad_norm_ returns the norm calculated BEFORE clipping\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            # --- MODIFICATION END ---\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            progress.update(1)\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                writer.add_scalar('train/loss', loss.item(), global_step)\n",
        "                writer.add_scalar('train/grad_norm', grad_norm.item(), global_step) # Log to TensorBoard\n",
        "\n",
        "                # Add 'gnorm' to the progress bar (formatted to 2 decimal places)\n",
        "                progress.set_postfix(loss=loss.item(), gnorm=f\"{grad_norm.item():.2f}\")\n",
        "\n",
        "            if global_step in VALIDATION_SCHEDULE:\n",
        "                logger.info(f\"Validating at step {global_step}...\")\n",
        "                current_bleu = evaluate(model, val_dataloader, device)\n",
        "                writer.add_scalar('val/bleu', current_bleu, global_step)\n",
        "                logger.info(f\"Step {global_step} | BLEU: {current_bleu:.4f}\")\n",
        "                if current_bleu > best_bleu:\n",
        "                    best_bleu = current_bleu\n",
        "                    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model.pt\"))\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"marathon_model.pt\"))\n",
        "    logger.info(f\"Marathon Complete. Best BLEU: {best_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xsWfDkeWp5-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "import torchmetrics\n",
        "import numpy\n",
        "import pkg_resources\n",
        "\n",
        "def log_environment_separate(log_dir):\n",
        "    # Define the separate file path\n",
        "    meta_file = os.path.join(log_dir, \"system_metadata.txt\")\n",
        "\n",
        "    with open(meta_file, \"w\") as f:\n",
        "        # --- PART 1: SUMMARY ---\n",
        "        f.write(\"=\"*40 + \"\\n\")\n",
        "        f.write(\"CORE ENVIRONMENT SUMMARY\\n\")\n",
        "        f.write(\"=\"*40 + \"\\n\")\n",
        "        f.write(f\"Python:       {sys.version.split()[0]}\\n\")\n",
        "        f.write(f\"PyTorch:      {torch.__version__}\\n\")\n",
        "        f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
        "        f.write(f\"Datasets:     {datasets.__version__}\\n\")\n",
        "        f.write(f\"TorchMetrics: {torchmetrics.__version__}\\n\")\n",
        "        f.write(f\"NumPy:        {numpy.__version__}\\n\")\n",
        "\n",
        "        try:\n",
        "            import sacrebleu\n",
        "            f.write(f\"SacreBLEU:    {sacrebleu.__version__}\\n\")\n",
        "        except ImportError:\n",
        "            f.write(\"SacreBLEU:    Not Installed\\n\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            f.write(f\"GPU Name:     {torch.cuda.get_device_name(0)}\\n\")\n",
        "            f.write(f\"CUDA Ver:     {torch.version.cuda}\\n\")\n",
        "            f.write(f\"Capability:   {torch.cuda.get_device_capability(0)}\\n\")\n",
        "        else:\n",
        "            f.write(\"GPU:          None (CPU Only)\\n\")\n",
        "\n",
        "        # --- PART 2: FULL FREEZE ---\n",
        "        f.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "        f.write(\"FULL LIBRARY DEPENDENCIES (PIP FREEZE)\\n\")\n",
        "        f.write(\"=\"*40 + \"\\n\")\n",
        "\n",
        "        installed_packages = {d.project_name: d.version for d in pkg_resources.working_set}\n",
        "        for package, version in sorted(installed_packages.items()):\n",
        "            f.write(f\"{package}=={version}\\n\")\n",
        "\n",
        "    print(f\"âœ… Environment details saved SEPARATELY to: {meta_file}\")\n",
        "\n",
        "# Execute\n",
        "# Assumes CURRENT_RUN_DIR is defined from your config\n",
        "log_environment_separate(CURRENT_RUN_DIR)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "mount_file_id": "1v6gT4yx0-mg1GwyhUz9GtW-SXMW7tN_g",
      "authorship_tag": "ABX9TyOVbvWegpx2xTcdL9Uai+Ob",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}